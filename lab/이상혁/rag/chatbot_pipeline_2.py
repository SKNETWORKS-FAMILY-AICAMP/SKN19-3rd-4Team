import os
import asyncio
import asyncpg
import json
import time
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer, CrossEncoder
from openai import OpenAI

# API í‚¤ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "YOUR_API_KEY_HERE")
if not OPENAI_API_KEY or OPENAI_API_KEY == "YOUR_API_KEY_HERE":
    print("âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ì½”ë“œì— ì§ì ‘ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

# DB ì„¤ì •
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'skn19_3rd_proj',
    'user': 'rag_user',
    'password': 'skn19'
}

# í´ë¼ì´ì–¸íŠ¸ ë° ëª¨ë¸ ë¡œë“œ
openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
embedding_model = SentenceTransformer('BAAI/bge-m3')

# CrossEncoder('Dongjin-kr/ko-reranker', device='cpu') - í•œêµ­ì–´ íŠ¹í™”, ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼
# CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2') - ë²”ìš©, ë¹ ë¦„
RERANKER = CrossEncoder('Dongjin-kr/ko-reranker', device='cpu')

print("í™˜ê²½ ì„¤ì • ì™„ë£Œ")


# =============================================================================
# ë©€í‹° ì¿¼ë¦¬ ìƒì„± í•¨ìˆ˜
# =============================================================================
def generate_multi_queries(query: str, num_queries: int = 3) -> List[str]:
    """ì›ë³¸ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê°œì˜ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    
    multi_query_prompt = """ë‹¹ì‹ ì€ LH ê³µì‚¬ ì„ëŒ€/ë¶„ì–‘ ê³µê³  ê²€ìƒ‰ì„ ë•ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì¬ì‘ì„±í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ ë†’ì´ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸ì— ëŒ€í•´ 3ê°œì˜ ë‹¤ë¥¸ ë²„ì „ì„ ìƒì„±í•˜ì„¸ìš”:
1. ë™ì˜ì–´ë‚˜ ìœ ì‚¬ í‘œí˜„ì„ ì‚¬ìš©í•œ ë²„ì „
2. ë” êµ¬ì²´ì ì´ê±°ë‚˜ ìƒì„¸í•œ ë²„ì „  
3. ë” ì¼ë°˜ì ì´ê±°ë‚˜ ë„“ì€ ë²”ìœ„ì˜ ë²„ì „

ê·œì¹™:
- ê° ì§ˆë¬¸ì€ í•œ ì¤„ì— í•˜ë‚˜ì”© ì‘ì„±
- ë²ˆí˜¸ë‚˜ ê¸°í˜¸ ì—†ì´ ì§ˆë¬¸ë§Œ ì‘ì„±
- ì›ë³¸ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ìœ ì§€
- LH, ì„ëŒ€ì£¼íƒ, ë¶„ì–‘ì£¼íƒ ê´€ë ¨ ìš©ì–´ í™œìš©

ì›ë³¸ ì§ˆë¬¸: {question}

ë³€í™˜ëœ ì§ˆë¬¸ë“¤:"""

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": multi_query_prompt.format(question=query)}],
            temperature=0.7,
            max_tokens=500
        )
        result = response.choices[0].message.content
        generated = [q.strip() for q in result.strip().split("\n") if q.strip()]
        return [query] + generated[:num_queries]
    except Exception as e:
        print(f"ë©€í‹° ì¿¼ë¦¬ ìƒì„± ì˜¤ë¥˜: {e}")
        return [query]


async def rewrite_query(query: str, conversation_history: List[Dict] = None) -> Dict:
    """LLMì„ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ê³  í™•ì¥"""
    
    context_str = ""
    if conversation_history:
        recent = conversation_history[-3:]
        context_str = "\nì´ì „ ëŒ€í™”:\n" + "\n".join([
            f"Q: {h['query']}\nA: {h['answer'][:100]}..." for h in recent
        ])
    
    system_prompt = """ë‹¹ì‹ ì€ LH ê³µê³  ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì§ˆë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”:

1. rewritten: ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì¬êµ¬ì„±ëœ ì§ˆë¬¸ (ëŒ€í™” ë§¥ë½ ë°˜ì˜)
2. expanded: ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ í™•ì¥ ì¿¼ë¦¬ (ìœ ì‚¬ì–´, ê´€ë ¨ì–´ í¬í•¨)
3. keywords: í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì„¸ë¶€ ì§€ì—­ëª… í¬í•¨)
4. filters: ë©”íƒ€ë°ì´í„° í•„í„°
   - region: "ê²½ê¸°ë„", "ì„œìš¸íŠ¹ë³„ì‹œ", "ì„œìš¸íŠ¹ë³„ì‹œ ì™¸" ì¤‘ í•˜ë‚˜
   - notice_type: "êµ­ë¯¼ì„ëŒ€", "í–‰ë³µì£¼íƒ", "ì˜êµ¬ì„ëŒ€" ë“±
   - category: "lease" ë˜ëŠ” "sale"

ì¤‘ìš”: ì„¸ë¶€ ì§€ì—­ëª…(ë‚¨ì–‘ì£¼, ìˆ˜ì›)ì€ keywordsì—ë§Œ, filters.regionì€ ê´‘ì—­ì‹œ/ë„ë§Œ ì‚¬ìš©"""

    user_prompt = f"{context_str}\n\ní˜„ì¬ ì§ˆë¬¸: {query}\n\nìœ„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”."

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        result = json.loads(response.choices[0].message.content)
        result['original'] = query
        return result
    except Exception as e:
        return {'original': query, 'rewritten': query, 'expanded': query, 'keywords': query.split(), 'filters': {}}


async def vector_search(query: str, top_k: int = 15, filters: dict = None, filter_ids: List[str] = None) -> List[Dict]:
    """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ (ì˜ë¯¸ ê¸°ë°˜)"""
    query_embedding = embedding_model.encode(query, normalize_embeddings=True)
    conn = await asyncpg.connect(**DB_CONFIG)
    
    try:
        where_clauses, params = [], [str(query_embedding.tolist())]
        
        if filters:
            if 'region' in filters:
                where_clauses.append(f"a.region LIKE ${len(params)+1}")
                params.append(f"%{filters['region']}%")
            if 'category' in filters:
                where_clauses.append(f"a.category = ${len(params)+1}")
                params.append(filters['category'])
            if 'notice_type' in filters:
                where_clauses.append(f"a.notice_type LIKE ${len(params)+1}")
                params.append(f"%{filters['notice_type']}%")
        
        if filter_ids:
            where_clauses.append(f"a.id = ANY(${len(params)+1}::text[])")
            params.append(filter_ids)
        
        where_sql = " AND " + " AND ".join(where_clauses) if where_clauses else ""
        params.append(top_k)
        
        sql = f"""
            SELECT dc.id as chunk_id, dc.announcement_id, a.title, a.category, a.region, a.notice_type,
                   dc.chunk_text, dc.metadata, (1 - (dc.embedding <=> $1::vector)) as similarity, 'vector' as search_type
            FROM document_chunks dc
            JOIN announcements a ON dc.announcement_id = a.id
            WHERE 1=1 {where_sql}
            ORDER BY dc.embedding <=> $1::vector
            LIMIT ${len(params)}
        """
        return [dict(r) for r in await conn.fetch(sql, *params)]
    finally:
        await conn.close()


async def keyword_search(keywords: List[str], top_k: int = 10, filters: dict = None) -> List[Dict]:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (LIKE ê²€ìƒ‰)"""
    conn = await asyncpg.connect(**DB_CONFIG)
    
    try:
        params, keyword_conditions = [], []
        for kw in keywords:
            keyword_conditions.append(f"dc.chunk_text LIKE ${len(params)+1}")
            params.append(f"%{kw}%")
        
        keyword_sql = " OR ".join(keyword_conditions) if keyword_conditions else "1=1"
        
        where_clauses = []
        if filters:
            if 'region' in filters:
                where_clauses.append(f"a.region LIKE ${len(params)+1}")
                params.append(f"%{filters['region']}%")
            if 'category' in filters:
                where_clauses.append(f"a.category = ${len(params)+1}")
                params.append(filters['category'])
            if 'notice_type' in filters:
                where_clauses.append(f"a.notice_type LIKE ${len(params)+1}")
                params.append(f"%{filters['notice_type']}%")
        
        where_sql = " AND " + " AND ".join(where_clauses) if where_clauses else ""
        params.append(top_k)
        
        sql = f"""
            SELECT DISTINCT ON (dc.id) dc.id as chunk_id, dc.announcement_id, a.title, a.category, a.region,
                   a.notice_type, dc.chunk_text, dc.metadata, 0.5 as similarity, 'keyword' as search_type
            FROM document_chunks dc
            JOIN announcements a ON dc.announcement_id = a.id
            WHERE ({keyword_sql}) {where_sql}
            LIMIT ${len(params)}
        """
        return [dict(r) for r in await conn.fetch(sql, *params)]
    finally:
        await conn.close()


# =============================================================================
# ë©€í‹° ì¿¼ë¦¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
# =============================================================================
async def multi_query_hybrid_search(
    query_analysis: Dict, 
    use_multi_query: bool = True,
    vector_top_k: int = 10, 
    keyword_top_k: int = 5
) -> Tuple[List[Dict], List[str]]:
    """ë©€í‹° ì¿¼ë¦¬ë¥¼ í™œìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
    
    if use_multi_query:
        queries = generate_multi_queries(query_analysis.get('rewritten', query_analysis['original']))
        print(f"  ğŸ“ ìƒì„±ëœ ì§ˆì˜ ({len(queries)}ê°œ):")
        for i, q in enumerate(queries):
            prefix = "ì›ë³¸" if i == 0 else f"ë³€í™˜{i}"
            print(f"     [{prefix}] {q}")
    else:
        queries = [query_analysis.get('rewritten', query_analysis['original'])]
    
    all_results, seen_chunks = [], set()
    filters = query_analysis.get('filters', {})
    
    for q in queries:
        vector_results = await vector_search(q, top_k=vector_top_k, filters=filters)
        for r in vector_results:
            if r['chunk_id'] not in seen_chunks:
                seen_chunks.add(r['chunk_id'])
                all_results.append(r)
        
        if q == queries[0]:
            keyword_results = await keyword_search(query_analysis.get('keywords', []), top_k=keyword_top_k, filters=filters)
            for r in keyword_results:
                if r['chunk_id'] not in seen_chunks:
                    seen_chunks.add(r['chunk_id'])
                    all_results.append(r)
    
    return all_results, queries


async def hybrid_search(query_analysis: Dict, vector_top_k: int = 15, keyword_top_k: int = 10) -> List[Dict]:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë‹¨ì¼ ì¿¼ë¦¬ ë²„ì „, í˜¸í™˜ì„± ìœ ì§€)"""
    results, _ = await multi_query_hybrid_search(query_analysis, use_multi_query=False, vector_top_k=vector_top_k, keyword_top_k=keyword_top_k)
    return results


# =============================================================================
# ë¦¬ë­í‚¹ í•¨ìˆ˜
# =============================================================================
def rerank_results(query: str, search_results: List[Dict], top_k: int = 8) -> List[Dict]:
    """Cross-Encoderë¥¼ ì‚¬ìš©í•œ ì •ë°€ ì¬ìˆœìœ„í™”"""
    if not search_results:
        return []
    
    pairs = [(query, r['chunk_text']) for r in search_results]
    scores = RERANKER.predict(pairs, show_progress_bar=False)
    
    for i, result in enumerate(search_results):
        result['rerank_score'] = float(scores[i])
    
    reranked = sorted(search_results, key=lambda x: x['rerank_score'], reverse=True)
    return reranked[:top_k]


def build_context(reranked_results: List[Dict]) -> Tuple[str, List[Dict]]:
    """ì²­í¬ ë³‘í•© ë° êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
    
    announcement_chunks = {}
    for r in reranked_results:
        ann_id = r['announcement_id']
        if ann_id not in announcement_chunks:
            announcement_chunks[ann_id] = {
                'announcement_id': ann_id, 'title': r['title'], 'category': r['category'],
                'region': r['region'], 'notice_type': r['notice_type'], 'metadata': r['metadata'],
                'chunk_texts': [r['chunk_text']], 'rerank_score': r['rerank_score'], 'chunk_count': 1
            }
        else:
            announcement_chunks[ann_id]['chunk_texts'].append(r['chunk_text'])
            announcement_chunks[ann_id]['chunk_count'] += 1
            announcement_chunks[ann_id]['rerank_score'] = max(announcement_chunks[ann_id]['rerank_score'], r['rerank_score'])
    
    merged = sorted(announcement_chunks.values(), key=lambda x: x['rerank_score'], reverse=True)
    
    context_parts, sources = [], []
    for idx, m in enumerate(merged, 1):
        metadata = json.loads(m['metadata']) if isinstance(m['metadata'], str) else m['metadata']
        category_name = "ì„ëŒ€" if m['category'] == 'lease' else "ë¶„ì–‘"
        merged_text = '\n\n'.join(m['chunk_texts'])
        
        context_parts.append(f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ë¬¸ì„œ {idx}: {m['title']}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[ê¸°ë³¸ ì •ë³´]
- ë¶„ë¥˜: {category_name}
- ì§€ì—­: {m['region']}
- ìœ í˜•: {m['notice_type'] or 'N/A'}
- ê´€ë ¨ë„: {m['rerank_score']:.3f}

[ë¬¸ì„œ ë‚´ìš©]
{merged_text}
        """.strip())
        
        # sourcesì— category, notice_type ì¶”ê°€
        sources.append({
            'announcement_id': m['announcement_id'],
            'title': m['title'],
            'category': m['category'],
            'region': m['region'],
            'notice_type': m['notice_type'],
            'score': m['rerank_score'],
            'chunk_count': m['chunk_count']
        })
    
    return "\n\n".join(context_parts), sources


def generate_answer(query: str, context: str, sources: List[Dict], queries_used: List[str] = None) -> Dict:
    """LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
    
    system_prompt = """ë‹¹ì‹ ì€ LH ê³µì‚¬ì˜ ì„ëŒ€/ë¶„ì–‘ ê³µê³  ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

# ë‹µë³€ ì›ì¹™
1. ì œê³µëœ ë¬¸ì„œë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ê³µê³ ì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ëª…ì‹œ
3. í‘œê°€ ìˆìœ¼ë©´ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ì •ë¦¬
4. ìˆ«ì, ë‚ ì§œ, ì¡°ê±´ì€ ì •í™•íˆ ì¸ìš©
5. ë‹µë³€ ëì— [ë¬¸ì„œ 1, 2 ì°¸ì¡°] í˜•íƒœë¡œ ì¶œì²˜ í‘œì‹œ"""

    user_prompt = f"# ì œê³µëœ ë¬¸ì„œ\n\n{context}\n\n# ì‚¬ìš©ì ì§ˆë¬¸\n{query}\n\nìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,
            max_tokens=2000
        )
        return {
            'answer': response.choices[0].message.content,
            'sources': sources,
            'queries_used': queries_used or [],
            'metadata': {'model': 'gpt-4o-mini', 'tokens': response.usage.total_tokens}
        }
    except Exception as e:
        return {'answer': f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}", 'sources': sources, 'queries_used': queries_used or [], 'metadata': {'error': str(e)}}


# =============================================================================
# ì°¸ê³  ë¬¸ì„œ ì¶œë ¥ í•¨ìˆ˜
# =============================================================================
def print_source_documents(sources: List[Dict]):
    """ì°¸ê³  ë¬¸ì„œë¥¼ í‘œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥"""
    print("\n" + "-"*80)
    confidence = "ë†’ìŒ" if len(sources) >= 3 else "ì¤‘ê°„" if len(sources) >= 1 else "ë‚®ìŒ"
    print(f"### ğŸ“š ì°¸ê³  ë¬¸ì„œ ({len(sources)}ê±´) | ì‹ ë¢°ë„: {confidence} ###")
    print("-"*80 + "\n")
    
    if sources:
        print("| ìˆœë²ˆ | ê³µê³ ëª… | ë¶„ë¥˜ | ì§€ì—­ | ê³µê³ ìœ í˜• | ê´€ë ¨ë„ |")
        print("|:---:|:---|:---:|:---:|:---:|:---:|")
        for i, doc in enumerate(sources, 1):
            title = doc['title'][:40] + "..." if len(doc['title']) > 40 else doc['title']
            category = "ì„ëŒ€" if doc.get('category') == 'lease' else "ë¶„ì–‘"
            region = doc.get('region', 'N/A')
            notice_type = doc.get('notice_type', 'N/A') if doc.get('notice_type') else 'N/A'
            score = f"{doc.get('score', 0):.3f}"
            print(f"| {i} | {title} | {category} | {region} | {notice_type} | {score} |")
    else:
        print("ì°¸ê³  ë¬¸ì„œ ì—†ìŒ")


# =============================================================================
# í†µí•© RAG íŒŒì´í”„ë¼ì¸ (ë©€í‹° ì¿¼ë¦¬ ì§€ì›)
# =============================================================================
async def rag_chatbot(
    query: str, 
    conversation_history: List[Dict] = None, 
    verbose: bool = True,
    use_multi_query: bool = True
) -> Dict:
    """6ë‹¨ê³„ RAG íŒŒì´í”„ë¼ì¸ í†µí•© í•¨ìˆ˜"""
    start_time = time.time()
    
    if verbose:
        print(f"\n{'='*80}\nì§ˆë¬¸: {query}\n{'='*80}")
        print(f"ğŸ”„ Multi-Query: {'í™œì„±í™”' if use_multi_query else 'ë¹„í™œì„±í™”'}")
    
    # 1. ì§ˆë¬¸ ì¬êµ¬ì„±
    step1_start = time.time()
    query_analysis = await rewrite_query(query, conversation_history)
    step1_time = time.time() - step1_start
    if verbose:
        print(f"\n[1/5] ì§ˆë¬¸ ì¬êµ¬ì„±: {query_analysis.get('rewritten', 'N/A')} ({step1_time:.2f}ì´ˆ)")
    
    # 2. ë©€í‹° ì¿¼ë¦¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    step2_start = time.time()
    search_results, queries_used = await multi_query_hybrid_search(query_analysis, use_multi_query=use_multi_query, vector_top_k=10, keyword_top_k=5)
    step2_time = time.time() - step2_start
    if verbose:
        print(f"[2/5] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: {len(search_results)}ê°œ ê²°ê³¼ ({step2_time:.2f}ì´ˆ)")
    
    if not search_results:
        return {'query': query, 'answer': "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", 'sources': [], 'queries_used': queries_used}
    
    # 3. ì¬ìˆœìœ„í™”
    step3_start = time.time()
    reranked = rerank_results(query_analysis.get('rewritten', query), search_results, top_k=8)
    step3_time = time.time() - step3_start
    if verbose:
        print(f"[3/5] ì¬ìˆœìœ„í™”: ìƒìœ„ {len(reranked)}ê°œ ì„ ì • (ìµœê³  ì ìˆ˜: {reranked[0]['rerank_score']:.4f}) ({step3_time:.2f}ì´ˆ)")
    
    # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    step4_start = time.time()
    context, sources = build_context(reranked)
    step4_time = time.time() - step4_start
    if verbose:
        print(f"[4/5] ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±: {len(context)} ë¬¸ì ({step4_time:.2f}ì´ˆ)")
    
    # 5. ë‹µë³€ ìƒì„±
    step5_start = time.time()
    result = generate_answer(query_analysis.get('rewritten', query), context, sources, queries_used)
    step5_time = time.time() - step5_start
    
    total_time = time.time() - start_time
    
    if verbose:
        print(f"[5/5] ë‹µë³€ ìƒì„± ì™„ë£Œ ({step5_time:.2f}ì´ˆ)")
        print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"   - ì§ˆë¬¸ ì¬êµ¬ì„±: {step1_time:.2f}ì´ˆ")
        print(f"   - ê²€ìƒ‰: {step2_time:.2f}ì´ˆ")
        print(f"   - ì¬ìˆœìœ„í™”: {step3_time:.2f}ì´ˆ")
        print(f"   - ì»¨í…ìŠ¤íŠ¸: {step4_time:.2f}ì´ˆ")
        print(f"   - ë‹µë³€ ìƒì„±: {step5_time:.2f}ì´ˆ")
        print(f"\n{'='*80}\n{result['answer']}\n{'='*80}")
        
        # ì°¸ê³  ë¬¸ì„œ ì¶œë ¥
        print_source_documents(sources)
        
        # ì‚¬ìš©ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ì¶œë ¥
        if queries_used and len(queries_used) > 1:
            print(f"\nğŸ” ì‚¬ìš©ëœ ê²€ìƒ‰ ì¿¼ë¦¬:")
            for i, q in enumerate(queries_used):
                prefix = "ì›ë³¸" if i == 0 else f"ë³€í™˜{i}"
                print(f"   [{prefix}] {q}")
    
    return {
        'query': query, 
        'query_analysis': query_analysis, 
        'queries_used': queries_used,
        'timing': {'total': total_time, 'rewrite': step1_time, 'search': step2_time, 'rerank': step3_time, 'context': step4_time, 'generate': step5_time},
        **result
    }


# =============================================================================
# ëŒ€í™” ë§¥ë½ ê´€ë¦¬
# =============================================================================
conversation_history = []

async def analyze_context(query: str, history: List[Dict]) -> Dict:
    """LLMìœ¼ë¡œ ë§¥ë½ ì°¸ì¡° ë¶„ì„"""
    if not history:
        return {'is_context_question': False}
    
    history_str = "\n".join([f"Q: {h['query']}\nA: {h['answer'][:200]}..." for h in history[-2:]])
    
    system_prompt = """ëŒ€í™” ë§¥ë½ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.
JSON ì‘ë‹µ: {"is_context_question": true/false, "reason": "íŒë‹¨ ê·¼ê±°", "referenced_announcement_indices": [0, 1]}"""
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ì´ì „ ëŒ€í™”:\n{history_str}\n\ní˜„ì¬ ì§ˆë¬¸: {query}"}
            ],
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except:
        return {'is_context_question': False}


async def chat(query: str, verbose: bool = True, use_multi_query: bool = True):
    """ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ëŠ” ì±—ë´‡"""
    
    context_analysis = await analyze_context(query, conversation_history)
    is_context = context_analysis.get('is_context_question', False)
    
    if is_context and conversation_history:
        if verbose:
            print(f"[ë§¥ë½ ì¸ì‹] {context_analysis.get('reason', '')}")
        
        prev_ids = []
        for idx in context_analysis.get('referenced_announcement_indices', [0]):
            if idx < len(conversation_history):
                for src in conversation_history[-(idx+1)].get('sources', [])[:3]:
                    if src.get('announcement_id') and src['announcement_id'] not in prev_ids:
                        prev_ids.append(src['announcement_id'])
        
        if prev_ids:
            query_analysis = await rewrite_query(query, conversation_history)
            
            context_results = await vector_search(query_analysis.get('rewritten', query), top_k=5, filter_ids=prev_ids)
            general_results, queries_used = await multi_query_hybrid_search(query_analysis, use_multi_query=use_multi_query)
            
            seen = {r['chunk_id'] for r in context_results}
            combined = context_results + [r for r in general_results if r['chunk_id'] not in seen]
            
            reranked = rerank_results(query_analysis.get('rewritten', query), combined, top_k=8)
            context, sources = build_context(reranked)
            result = generate_answer(query_analysis.get('rewritten', query), context, sources, queries_used)
            result = {'query': query, 'query_analysis': query_analysis, **result}
            
            if verbose:
                print(f"\n{'='*80}\n{result['answer']}\n{'='*80}")
                # ì°¸ê³  ë¬¸ì„œ ì¶œë ¥
                print_source_documents(sources)
        else:
            result = await rag_chatbot(query, conversation_history, verbose, use_multi_query)
    else:
        result = await rag_chatbot(query, conversation_history, verbose, use_multi_query)
    
    conversation_history.append({
        'query': query,
        'answer': result['answer'],
        'sources': result.get('sources', [])
    })
    
    if len(conversation_history) > 10:
        conversation_history.pop(0)
    
    return result


# =============================================================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# =============================================================================
async def main():
    print("\n" + "="*80)
    print("ğŸ“Œ í…ŒìŠ¤íŠ¸ 1: ë©€í‹° ì¿¼ë¦¬ í™œì„±í™”")
    print("="*80)
    await chat("ìˆ˜ì›ì‹œ í–‰ë³µì£¼íƒ ì•Œë ¤ì¤˜", use_multi_query=True)
    
    print("\n" + "="*80)
    print("ğŸ“Œ í…ŒìŠ¤íŠ¸ 2: ë§¥ë½ ì°¸ì¡° ì§ˆë¬¸")
    print("="*80)
    await chat("ê±°ê¸° ì²­ë…„ ê³„ì¸µ ì¶œìƒìë…€ì— ë”°ë¥¸ ì†Œë“ ê¸°ì¤€ì€?", use_multi_query=True)
    
    print("\n" + "="*80)
    print("ğŸ“Œ í…ŒìŠ¤íŠ¸ 3: ë©€í‹° ì¿¼ë¦¬ ë¹„í™œì„±í™” (ë¹„êµ)")
    print("="*80)
    conversation_history.clear()
    await chat("LH í–‰ë³µì£¼íƒ ì²­ë…„ ëŒ€ìƒ ì¡°ê±´ì€?", use_multi_query=False)


if __name__ == "__main__":
    asyncio.run(main())