## 🏠 LH 공사 임대/분양 공고 RAG 파이프라인 아키텍처 (상세 분석)

제공해주신 아키텍처 설계도를 바탕으로, **Multi-Query**와 **Hybrid Search**를 결합한 LH 공사 RAG 파이프라인의 각 단계를 상세히 분석하고 사용된 기술 스택을 정리합니다. 이 파이프라인은 검색의 **범용성(Recall)**과 **정확성(Precision)**을 극대화하는 것을 목표로 합니다.

---

## 1. 전체 아키텍처 개요

이 RAG 파이프라인은 사용자의 질문 하나를 **4개의 질의**로 확장하고, **Dense/Sparse 검색**으로 문서를 확보한 뒤, **Reranking**을 통해 가장 관련성 높은 5개 문서를 선별하여 **GPT-4o-mini**로 구조화된 답변을 생성하는 7단계(0~5단계) 구조를 가집니다.

| 단계 | 명칭 | 핵심 기술 | 목표 |
| :---: | :--- | :--- | :--- |
| **0** | **Multi-Query 생성** | GPT-4o-mini (T=0.7) | 검색 질의 확장 (Recall 증대) |
| **1** | **Hybrid Retrieval** | PGVector + BM25 | 의미 및 키워드 기반의 광범위한 문서 확보 |
| **2** | **Reranking** | CrossEncoder (MiniLM-L-6-v2) | 최종 후보 문서의 정확성(Precision) 극대화 |
| **3** | **Prompt Engineering** | 동적 질문 분석 | LLM 답변의 품질, 형식, 역할 정의 |
| **4** | **Generation** | GPT-4o-mini (T=0.2) | 최종 답변 생성 |
| **5** | **Response Formatting** | Pydantic | 응답의 구조화 및 일관성 확보 |

---

## 2. Stage 0: Multi-Query 생성

### 2.1. 개념 및 역할

사용자의 **원본 질문**을 **LLM(Large Language Model)**을 이용해 의미가 유사하거나 확장된 **3개의 추가 질의**로 변환합니다. 최종적으로 원본 포함 4개의 질의를 검색에 사용합니다.

* **목표**: 단일 질의가 특정 키워드나 문맥에 갇히는 문제를 해결하고, 검색 엔진이 더 많은 **관련 문서(Recall)**를 가져올 수 있도록 검색 범위를 확장합니다.

### 2.2. 사용 기술

| 항목 | 내용 |
| :--- | :--- |
| **라이브러리** | `langchain_openai`, `langchain_core` |
| **LLM** | **GPT-4o-mini** |
| **Temperature** | **$0.7$** (다양하고 창의적인 표현 생성을 위해 높게 설정) |
| **프롬프트** | `MULTI_QUERY_PROMPT`를 사용하여 3가지 유형(유사어, 구체적, 일반적)의 질의 생성을 유도합니다. |

**예시**:
* 원본: "LH 행복주택 청년 조건이 뭐야?"
* 변환 2: **"청년 대상 LH 임대주택 신청 조건 및 자격 기준"** (더 구체적인 버전)

---

## 3. Stage 1: Hybrid Retrieval (하이브리드 검색)

### 3.1. 개념 및 역할

Multi-Query를 통해 생성된 **4개의 질의** 각각에 대해 **Dense Search (의미 기반)**와 **Sparse Search (키워드 기반)**를 병렬로 실행하여 후보 문서를 넓게 수집합니다.

### 3.2. Dense Retriever (PGVector)

| 항목 | 내용 | 특징 |
| :--- | :--- | :--- |
| **기술** | **PGVector** (PostgreSQL 확장) | 대규모 임베딩 저장 및 검색에 안정적입니다. |
| **임베딩** | **OpenAI Embeddings** (`text-embedding-3-small`) | 1536차원 벡터로 텍스트의 **의미적 문맥**을 인코딩합니다. |
| **거리 측정** | **Cosine Similarity** | 벡터 공간에서의 방향 유사도를 측정하여 관련성을 판단합니다. |
| **검색 결과** | 질의당 $\text{top-}k = 10$개 $\rightarrow$ Multi-Query 적용 시 최대 $40$개 | **의미적 유사성**을 기반으로 문맥에 맞는 문서를 찾습니다. |

### 3.3. Sparse Retriever (BM25)

| 항목 | 내용 | 특징 |
| :--- | :--- | :--- |
| **기술** | **BM25Okapi** (`rank_bm25` 라이브러리) | **정확한 키워드 매칭**과 **키워드의 희귀도**에 기반하여 점수를 부여합니다. |
| **DB 접근** | **`asyncpg`** | 비동기 방식으로 DB에서 모든 문서 청크와 메타데이터를 로드하여 **BM25 인덱스**를 생성합니다. |
| **검색 원리** | **TF-IDF** 기반의 발전된 통계 모델 | '조건', '자격'과 같이 문서에 정확히 명시된 키워드를 포함하는 문서를 효과적으로 찾습니다. |
| **검색 결과** | 질의당 $\text{top-}k = 5$개 $\rightarrow$ Multi-Query 적용 시 최대 $20$개 |

### 3.4. 통합 및 정제

1.  **결합**: Dense (20개)와 Sparse (20개) 결과를 합쳐 최대 40개의 후보 문서를 확보합니다.
2.  **중복 제거**: `page_content`를 기준으로 문서 중복을 제거하여 최종 후보 문서를 약 $15 \sim 25$개로 압축합니다.

---

## 4. Stage 2: Reranking (재정렬)

### 4.1. 개념 및 역할

Hybrid Search로 확보한 약 15~25개의 문서에 대해 **Cross-Encoder**를 사용하여 **질문과 문서 간의 상호작용**을 모델링하여 순위를 재조정하고, **정확도(Precision)**를 극대화합니다.

* **목표**: 초기 검색 점수(벡터 거리 또는 BM25 점수)가 높았더라도 실제 문맥적 관련성이 낮은 문서를 걸러내고, 최종적으로 가장 정확한 **상위 5개 문서**만 LLM에게 전달합니다. 

### 4.2. 사용 기술

| 항목 | 내용 | 특징 |
| :--- | :--- | :--- |
| **모델** | **`cross-encoder/ms-marco-MiniLM-L-6-v2`** | MS-MARCO 데이터셋으로 학습된 경량화된 Cross-Encoder 모델로, 빠르면서도 높은 재정렬 성능을 제공합니다. |
| **모델 유형** | **Cross-Encoder** | **(질문 + 문서)**를 한 번에 인코딩하여 두 입력 간의 상호 관련성 점수를 직접 계산합니다. (Bi-Encoder보다 정확하지만 느림) |
| **최종 결과** | $\text{top-}k = 5$개 |

---

## 5. Stage 3: Prompt Engineering (프롬프트 엔지니어링)

### 5.1. 개념 및 역할

LLM이 **정확하고 일관된 답변**을 생성하도록 역할을 규정하고, 검색된 문서를 효과적으로 통합하며, 질문 유형에 따라 **동적 지시사항**을 추가하는 단계입니다.

### 5.2. 구성 요소

| 항목 | 상세 설명 | 구현 기술 |
| :--- | :--- | :--- |
| **시스템 프롬프트** | LLM의 **역할**("LH 공고 전문 상담사"), **답변 원칙**(정확성, 명확성), **답변 형식**(마크다운, 강조 사용)을 정의하여 출력 일관성을 확보합니다. | `SYSTEM_PROMPT` |
| **동적 질문 분석** | `analyze_question_type` 함수가 질문 키워드("자격", "일정", "비용" 등)를 감지하고, **"자격 조건을 구체적으로 나열하세요"**와 같은 **추가 지시사항**을 생성하여 시스템 프롬프트에 추가합니다. | `analyze_question_type` 함수 |
| **컨텍스트 포맷팅** | 검색된 5개 문서 각각에 **공고명, 분류, 지역** 등의 **메타데이터**를 붙여서 LLM에게 **신뢰할 수 있는 출처 정보**와 함께 문서를 제공합니다. | `format_context` 함수 |

---

## 6. Stage 4: Generation (답변 생성)

### 6.1. 개념 및 역할

3단계에서 완성된 **최종 프롬프트**를 LLM에 전달하여 사용자의 질문에 대한 최종 답변 텍스트를 생성합니다.

### 6.2. 사용 기술

| 항목 | 내용 |
| :--- | :--- |
| **LLM** | **GPT-4o-mini** |
| **Temperature** | **$0.2$** (환각을 줄이고, 제공된 문서에만 기반한 **일관되고 사실적인** 답변을 유도하기 위해 낮게 설정) |

---

## 7. Stage 5: Response Formatting (응답 구조화)

### 7.1. 개념 및 역할

LLM이 생성한 답변 텍스트와 파이프라인 정보를 **구조화된 객체**로 정리하여 응답의 처리 용이성과 일관성을 극대화합니다.

### 7.2. Pydantic 모델 및 신뢰도

* **모델**: **`AnswerResponse`** 모델을 사용하여 응답에 `answer` (답변 텍스트), `source_documents` (참고 문서), `confidence` (신뢰도), `queries_used` (사용된 질의) 필드를 포함합니다.
* **신뢰도**: `reranked_docs`의 개수를 기준으로 신뢰도(`높음`, `중간`, `낮음`)를 계산하여 답변의 품질을 사용자에게 간접적으로 전달합니다.

| 문서 수 | 신뢰도 | 판단 기준 |
| :---: | :---: | :--- |
| $\ge 3$개 | **높음** | 충분한 근거 문서 확보 |
| $1 \sim 2$개 | **중간** | 일부 근거 존재, 추가 확인 권장 |
| $0$개 | **낮음** | 근거 부족 (질문과 관련된 문서 없음) |

---

## 8. 기술 스택 요약 및 데이터 흐름

### 8.1. 기술 스택 요약

| 카테고리 | 주요 기술/라이브러리 | 역할 |
| :--- | :--- | :--- |
| **LLM** | OpenAI (GPT-4o-mini) | 답변 생성, Multi-Query 생성 |
| **Dense Search** | OpenAI Embeddings, PGVector, Cosine Similarity | 의미 기반 검색 |
| **Sparse Search** | BM25 (rank-bm25) | 키워드 기반 검색 |
| **Reranking** | CrossEncoder (sentence-transformers) | 검색 결과 정밀 재정렬 |
| **프레임워크** | LangChain Core | RAG 파이프라인 체인 및 프롬프트 관리 |
| **데이터 구조** | Pydantic | 응답 구조화 및 검증 |
| **DB 연결** | asyncpg, psycopg2 | PostgreSQL 비동기/동기 연결 관리 |

### 8.2. 데이터 흐름 요약

| 단계 | Input | Process | Output |
| :--- | :--- | :--- | :--- |
| **0** | 사용자 질문 | LLM으로 질문 변환 | **4개 질의** |
| **1** | 4개 질의 | Dense/Sparse Search 결합 & 중복 제거 | **후보 문서 $15\sim 25$개** |
| **2** | 후보 문서, 원본 질문 | CrossEncoder 재정렬 | **최종 $\text{top-}5$ 문서** |
| **3** | $\text{top-}5$ 문서, 원본 질문 | 시스템 프롬프트 및 동적 지시사항 구성 | **최종 프롬프트** |
| **4** | 최종 프롬프트 | LLM 호출 | **마크다운 형식 답변** |
| **5** | LLM 답변, 문서 메타데이터 | Pydantic 구조화, 신뢰도 계산 | **`AnswerResponse` 객체** |