{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 챗봇 파이프라인\n",
    "\n",
    "LH 공고 검색을 위한 5단계 RAG 파이프라인:\n",
    "1. 질문 재구성 (Query Rewriting)\n",
    "2. 하이브리드 검색 (Vector + Keyword)\n",
    "3. 재순위화 (Reranking)\n",
    "4. 컨텍스트 구성 (청크 병합 + 구조화)\n",
    "5. 답변 생성 (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import asyncpg\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from openai import OpenAI\n",
    "\n",
    "# API 키 로드\n",
    "env_file = '.env'\n",
    "with open(env_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('OPENAI_API_KEY='):\n",
    "            os.environ['OPENAI_API_KEY'] = line.strip().split('=', 1)[1]\n",
    "            break\n",
    "\n",
    "# DB 설정\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'skn19_3rd_proj',\n",
    "    'user': 'rag_user',\n",
    "    'password': 'skn19'\n",
    "}\n",
    "\n",
    "# 클라이언트 및 모델 로드\n",
    "openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "embedding_model = SentenceTransformer('BAAI/bge-m3')\n",
    "reranker = CrossEncoder('Dongjin-kr/ko-reranker', device='cpu')\n",
    "\n",
    "print(\"환경 설정 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1단계: 질문 재구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rewrite_query(query: str, conversation_history: List[Dict] = None) -> Dict:\n",
    "    \"\"\"LLM을 활용하여 질문을 재구성하고 확장\"\"\"\n",
    "    \n",
    "    context_str = \"\"\n",
    "    if conversation_history:\n",
    "        recent = conversation_history[-3:]\n",
    "        context_str = \"\\n이전 대화:\\n\" + \"\\n\".join([\n",
    "            f\"Q: {h['query']}\\nA: {h['answer'][:100]}...\"\n",
    "            for h in recent\n",
    "        ])\n",
    "    \n",
    "    system_prompt = \"\"\"당신은 LH 공고 검색 시스템의 질문 분석 전문가입니다.\n",
    "사용자의 질문을 분석하여 다음 정보를 JSON 형식으로 추출하세요:\n",
    "\n",
    "1. rewritten: 완전한 문장으로 재구성된 질문 (대화 맥락 반영)\n",
    "2. expanded: 검색 최적화를 위한 확장 쿼리 (유사어, 관련어 포함)\n",
    "3. keywords: 핵심 키워드 리스트 (세부 지역명 포함)\n",
    "4. filters: 메타데이터 필터\n",
    "   - region: \"경기도\", \"서울특별시\", \"서울특별시 외\" 중 하나\n",
    "   - notice_type: \"국민임대\", \"행복주택\", \"영구임대\" 등\n",
    "   - category: \"lease\" 또는 \"sale\"\n",
    "\n",
    "중요: 세부 지역명(남양주, 수원)은 keywords에만, filters.region은 광역시/도만 사용\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"{context_str}\\n\\n현재 질문: {query}\\n\\n위 질문을 분석하여 검색에 최적화된 형태로 재구성해주세요.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        result['original'] = query\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {'original': query, 'rewritten': query, 'expanded': query, 'keywords': query.split(), 'filters': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2단계: 하이브리드 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def vector_search(query: str, top_k: int = 15, filters: dict = None, filter_ids: List[str] = None) -> List[Dict]:\n",
    "    \"\"\"벡터 유사도 검색 (의미 기반)\"\"\"\n",
    "    query_embedding = embedding_model.encode(query, normalize_embeddings=True)\n",
    "    conn = await asyncpg.connect(**DB_CONFIG)\n",
    "    \n",
    "    try:\n",
    "        where_clauses, params = [], [str(query_embedding.tolist())]\n",
    "        \n",
    "        if filters:\n",
    "            if 'region' in filters:\n",
    "                where_clauses.append(f\"a.region LIKE ${len(params)+1}\")\n",
    "                params.append(f\"%{filters['region']}%\")\n",
    "            if 'category' in filters:\n",
    "                where_clauses.append(f\"a.category = ${len(params)+1}\")\n",
    "                params.append(filters['category'])\n",
    "            if 'notice_type' in filters:\n",
    "                where_clauses.append(f\"a.notice_type LIKE ${len(params)+1}\")\n",
    "                params.append(f\"%{filters['notice_type']}%\")\n",
    "        \n",
    "        if filter_ids:\n",
    "            where_clauses.append(f\"a.id = ANY(${len(params)+1}::text[])\")\n",
    "            params.append(filter_ids)\n",
    "        \n",
    "        where_sql = \" AND \" + \" AND \".join(where_clauses) if where_clauses else \"\"\n",
    "        params.append(top_k)\n",
    "        \n",
    "        sql = f\"\"\"\n",
    "            SELECT dc.id as chunk_id, dc.announcement_id, a.title, a.category, a.region, a.notice_type,\n",
    "                   dc.chunk_text, dc.metadata, (1 - (dc.embedding <=> $1::vector)) as similarity, 'vector' as search_type\n",
    "            FROM document_chunks dc\n",
    "            JOIN announcements a ON dc.announcement_id = a.id\n",
    "            WHERE 1=1 {where_sql}\n",
    "            ORDER BY dc.embedding <=> $1::vector\n",
    "            LIMIT ${len(params)}\n",
    "        \"\"\"\n",
    "        return [dict(r) for r in await conn.fetch(sql, *params)]\n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "\n",
    "async def keyword_search(keywords: List[str], top_k: int = 10, filters: dict = None) -> List[Dict]:\n",
    "    \"\"\"키워드 기반 검색 (LIKE 검색)\"\"\"\n",
    "    conn = await asyncpg.connect(**DB_CONFIG)\n",
    "    \n",
    "    try:\n",
    "        params, keyword_conditions = [], []\n",
    "        for kw in keywords:\n",
    "            keyword_conditions.append(f\"dc.chunk_text LIKE ${len(params)+1}\")\n",
    "            params.append(f\"%{kw}%\")\n",
    "        \n",
    "        keyword_sql = \" OR \".join(keyword_conditions) if keyword_conditions else \"1=1\"\n",
    "        \n",
    "        where_clauses = []\n",
    "        if filters:\n",
    "            if 'region' in filters:\n",
    "                where_clauses.append(f\"a.region LIKE ${len(params)+1}\")\n",
    "                params.append(f\"%{filters['region']}%\")\n",
    "            if 'category' in filters:\n",
    "                where_clauses.append(f\"a.category = ${len(params)+1}\")\n",
    "                params.append(filters['category'])\n",
    "            if 'notice_type' in filters:\n",
    "                where_clauses.append(f\"a.notice_type LIKE ${len(params)+1}\")\n",
    "                params.append(f\"%{filters['notice_type']}%\")\n",
    "        \n",
    "        where_sql = \" AND \" + \" AND \".join(where_clauses) if where_clauses else \"\"\n",
    "        params.append(top_k)\n",
    "        \n",
    "        sql = f\"\"\"\n",
    "            SELECT DISTINCT ON (dc.id) dc.id as chunk_id, dc.announcement_id, a.title, a.category, a.region,\n",
    "                   a.notice_type, dc.chunk_text, dc.metadata, 0.5 as similarity, 'keyword' as search_type\n",
    "            FROM document_chunks dc\n",
    "            JOIN announcements a ON dc.announcement_id = a.id\n",
    "            WHERE ({keyword_sql}) {where_sql}\n",
    "            LIMIT ${len(params)}\n",
    "        \"\"\"\n",
    "        return [dict(r) for r in await conn.fetch(sql, *params)]\n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "\n",
    "async def hybrid_search(query_analysis: Dict, vector_top_k: int = 15, keyword_top_k: int = 10) -> List[Dict]:\n",
    "    \"\"\"하이브리드 검색: Vector + Keyword 결과를 병합\"\"\"\n",
    "    vector_results = await vector_search(query_analysis['expanded'], top_k=vector_top_k, filters=query_analysis.get('filters', {}))\n",
    "    keyword_results = await keyword_search(query_analysis.get('keywords', []), top_k=keyword_top_k, filters=query_analysis.get('filters', {}))\n",
    "    \n",
    "    seen_chunks, combined = set(), []\n",
    "    for r in vector_results + keyword_results:\n",
    "        if r['chunk_id'] not in seen_chunks:\n",
    "            seen_chunks.add(r['chunk_id'])\n",
    "            combined.append(r)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3단계: 재순위화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(query: str, search_results: List[Dict], top_k: int = 8) -> List[Dict]:\n",
    "    \"\"\"Cross-Encoder를 사용한 정밀 재순위화\"\"\"\n",
    "    if not search_results:\n",
    "        return []\n",
    "    \n",
    "    pairs = [(query, r['chunk_text']) for r in search_results]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    for i, result in enumerate(search_results):\n",
    "        result['rerank_score'] = float(scores[i])\n",
    "    \n",
    "    reranked = sorted(search_results, key=lambda x: x['rerank_score'], reverse=True)\n",
    "    return reranked[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4단계: 컨텍스트 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(reranked_results: List[Dict]) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"청크 병합 및 구조화된 컨텍스트 구성\"\"\"\n",
    "    \n",
    "    # 같은 공고의 청크들을 병합\n",
    "    announcement_chunks = {}\n",
    "    for r in reranked_results:\n",
    "        ann_id = r['announcement_id']\n",
    "        if ann_id not in announcement_chunks:\n",
    "            announcement_chunks[ann_id] = {\n",
    "                'announcement_id': ann_id, 'title': r['title'], 'category': r['category'],\n",
    "                'region': r['region'], 'notice_type': r['notice_type'], 'metadata': r['metadata'],\n",
    "                'chunk_texts': [r['chunk_text']], 'rerank_score': r['rerank_score'], 'chunk_count': 1\n",
    "            }\n",
    "        else:\n",
    "            announcement_chunks[ann_id]['chunk_texts'].append(r['chunk_text'])\n",
    "            announcement_chunks[ann_id]['chunk_count'] += 1\n",
    "            announcement_chunks[ann_id]['rerank_score'] = max(announcement_chunks[ann_id]['rerank_score'], r['rerank_score'])\n",
    "    \n",
    "    merged = sorted(announcement_chunks.values(), key=lambda x: x['rerank_score'], reverse=True)\n",
    "    \n",
    "    # 구조화된 컨텍스트 생성\n",
    "    context_parts, sources = [], []\n",
    "    for idx, m in enumerate(merged, 1):\n",
    "        metadata = json.loads(m['metadata']) if isinstance(m['metadata'], str) else m['metadata']\n",
    "        category_name = \"임대\" if m['category'] == 'lease' else \"분양\"\n",
    "        merged_text = '\\n\\n'.join(m['chunk_texts'])\n",
    "        \n",
    "        context_parts.append(f\"\"\"\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "문서 {idx}: {m['title']}\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "[기본 정보]\n",
    "- 분류: {category_name}\n",
    "- 지역: {m['region']}\n",
    "- 유형: {m['notice_type'] or 'N/A'}\n",
    "- 관련도: {m['rerank_score']:.3f}\n",
    "\n",
    "[문서 내용]\n",
    "{merged_text}\n",
    "        \"\"\".strip())\n",
    "        \n",
    "        sources.append({\n",
    "            'announcement_id': m['announcement_id'],\n",
    "            'title': m['title'],\n",
    "            'region': m['region'],\n",
    "            'score': m['rerank_score'],\n",
    "            'chunk_count': m['chunk_count']\n",
    "        })\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts), sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5단계: 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, context: str, sources: List[Dict]) -> Dict:\n",
    "    \"\"\"LLM으로 답변 생성\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"당신은 LH 공사의 임대/분양 공고 전문 상담사입니다.\n",
    "\n",
    "# 답변 원칙\n",
    "1. 제공된 문서만을 근거로 답변\n",
    "2. 문서에 없는 내용은 \"제공된 공고에서 확인할 수 없습니다\" 명시\n",
    "3. 표가 있으면 마크다운 표로 정리\n",
    "4. 숫자, 날짜, 조건은 정확히 인용\n",
    "5. 답변 끝에 [문서 1, 2 참조] 형태로 출처 표시\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"# 제공된 문서\\n\\n{context}\\n\\n# 사용자 질문\\n{query}\\n\\n위 문서를 바탕으로 정확하게 답변해주세요.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "        return {\n",
    "            'answer': response.choices[0].message.content,\n",
    "            'sources': sources,\n",
    "            'metadata': {'model': 'gpt-4o-mini', 'tokens': response.usage.total_tokens}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'answer': f\"답변 생성 오류: {str(e)}\", 'sources': sources, 'metadata': {'error': str(e)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통합 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rag_chatbot(query: str, conversation_history: List[Dict] = None, verbose: bool = True) -> Dict:\n",
    "    \"\"\"5단계 RAG 파이프라인 통합 함수\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\\n질문: {query}\\n{'='*80}\")\n",
    "    \n",
    "    # 1. 질문 재구성\n",
    "    query_analysis = await rewrite_query(query, conversation_history)\n",
    "    if verbose:\n",
    "        print(f\"\\n[1/5] 질문 재구성: {query_analysis.get('rewritten', 'N/A')}\")\n",
    "    \n",
    "    # 2. 하이브리드 검색\n",
    "    search_results = await hybrid_search(query_analysis, vector_top_k=15, keyword_top_k=10)\n",
    "    if verbose:\n",
    "        print(f\"[2/5] 하이브리드 검색: {len(search_results)}개 결과\")\n",
    "    \n",
    "    if not search_results:\n",
    "        return {'query': query, 'answer': \"관련 정보를 찾을 수 없습니다.\", 'sources': []}\n",
    "    \n",
    "    # 3. 재순위화\n",
    "    reranked = rerank_results(query_analysis.get('rewritten', query), search_results, top_k=8)\n",
    "    if verbose:\n",
    "        print(f\"[3/5] 재순위화: 상위 {len(reranked)}개 선정 (최고 점수: {reranked[0]['rerank_score']:.4f})\")\n",
    "    \n",
    "    # 4. 컨텍스트 구성\n",
    "    context, sources = build_context(reranked)\n",
    "    if verbose:\n",
    "        print(f\"[4/5] 컨텍스트 구성: {len(context)} 문자\")\n",
    "    \n",
    "    # 5. 답변 생성\n",
    "    result = generate_answer(query_analysis.get('rewritten', query), context, sources)\n",
    "    if verbose:\n",
    "        print(f\"[5/5] 답변 생성 완료\\n\\n{'='*80}\\n{result['answer']}\\n{'='*80}\")\n",
    "    \n",
    "    return {'query': query, 'query_analysis': query_analysis, **result}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대화형 챗봇 (맥락 유지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "async def analyze_context(query: str, history: List[Dict]) -> Dict:\n",
    "    \"\"\"LLM으로 맥락 참조 분석\"\"\"\n",
    "    if not history:\n",
    "        return {'is_context_question': False}\n",
    "    \n",
    "    history_str = \"\\n\".join([f\"Q: {h['query']}\\nA: {h['answer'][:200]}...\" for h in history[-2:]])\n",
    "    \n",
    "    system_prompt = \"\"\"대화 맥락 분석 전문가입니다. 현재 질문이 이전 대화를 참조하는지 판단하세요.\n",
    "JSON 응답: {\"is_context_question\": true/false, \"reason\": \"판단 근거\", \"referenced_announcement_indices\": [0, 1]}\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"이전 대화:\\n{history_str}\\n\\n현재 질문: {query}\"}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except:\n",
    "        return {'is_context_question': False}\n",
    "\n",
    "\n",
    "async def chat(query: str, verbose: bool = True):\n",
    "    \"\"\"대화 맥락을 유지하는 챗봇\"\"\"\n",
    "    \n",
    "    # 맥락 분석\n",
    "    context_analysis = await analyze_context(query, conversation_history)\n",
    "    is_context = context_analysis.get('is_context_question', False)\n",
    "    \n",
    "    if is_context and conversation_history:\n",
    "        if verbose:\n",
    "            print(f\"[맥락 인식] {context_analysis.get('reason', '')}\")\n",
    "        \n",
    "        # 이전 공고 ID 추출\n",
    "        prev_ids = []\n",
    "        for idx in context_analysis.get('referenced_announcement_indices', [0]):\n",
    "            if idx < len(conversation_history):\n",
    "                for src in conversation_history[-(idx+1)].get('sources', [])[:3]:\n",
    "                    if src.get('announcement_id') and src['announcement_id'] not in prev_ids:\n",
    "                        prev_ids.append(src['announcement_id'])\n",
    "        \n",
    "        if prev_ids:\n",
    "            # 질문 재구성\n",
    "            query_analysis = await rewrite_query(query, conversation_history)\n",
    "            \n",
    "            # 이전 공고에서 우선 검색\n",
    "            context_results = await vector_search(query_analysis.get('rewritten', query), top_k=5, filter_ids=prev_ids)\n",
    "            general_results = await hybrid_search(query_analysis, vector_top_k=15, keyword_top_k=10)\n",
    "            \n",
    "            # 결과 병합\n",
    "            seen = {r['chunk_id'] for r in context_results}\n",
    "            combined = context_results + [r for r in general_results if r['chunk_id'] not in seen]\n",
    "            \n",
    "            # 재순위화 및 답변 생성\n",
    "            reranked = rerank_results(query_analysis.get('rewritten', query), combined, top_k=8)\n",
    "            context, sources = build_context(reranked)\n",
    "            result = generate_answer(query_analysis.get('rewritten', query), context, sources)\n",
    "            result = {'query': query, 'query_analysis': query_analysis, **result}\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n{'='*80}\\n{result['answer']}\\n{'='*80}\")\n",
    "        else:\n",
    "            result = await rag_chatbot(query, conversation_history, verbose)\n",
    "    else:\n",
    "        result = await rag_chatbot(query, conversation_history, verbose)\n",
    "    \n",
    "    # 대화 이력 추가\n",
    "    conversation_history.append({\n",
    "        'query': query,\n",
    "        'answer': result['answer'],\n",
    "        'sources': result.get('sources', [])\n",
    "    })\n",
    "    \n",
    "    if len(conversation_history) > 10:\n",
    "        conversation_history.pop(0)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 질문 테스트\n",
    "result = await rag_chatbot(\"남양주시 국민임대주택 청약저축 납입 횟수 배점은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화형 테스트\n",
    "await chat(\"수원시 행복주택 알려줘\")\n",
    "await chat(\"거기 청년 계층 출생자녀에 따른 소득 기준은?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
