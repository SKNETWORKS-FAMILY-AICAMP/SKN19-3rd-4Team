{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ì¸¡ì •\n",
    "\n",
    "ë³¸ ë…¸íŠ¸ë¶ì€ ZIPFIT RAG ì‹œìŠ¤í…œì˜ 7ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì„ ë‹¨ê³„ë³„ë¡œ í…ŒìŠ¤íŠ¸í•˜ê³  ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "## í…ŒìŠ¤íŠ¸ ëª©í‘œ\n",
    "\n",
    "1. **ë©€í‹°ì¿¼ë¦¬ ìƒì„±**: ëª¨í˜¸í•œ ì§ˆë¬¸ì´ 3~4ê°œë¡œ í™•ì¥ë˜ëŠ”ì§€ ê²€ì¦\n",
    "2. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: ë²¡í„° + í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸\n",
    "3. **ë¦¬ë­í‚¹ & ë³‘í•©**: ì¬ìˆœìœ„í™” ë° ì²­í¬ ë³‘í•© íš¨ê³¼ ê²€ì¦\n",
    "4. **ë‹µë³€ ìƒì„±**: í™˜ê° ì—†ëŠ” ë‹µë³€ ìƒì„± í™•ì¸\n",
    "\n",
    "## ì„±ëŠ¥ ëª©í‘œ\n",
    "\n",
    "- ì „ì²´ íŒŒì´í”„ë¼ì¸: 5~10ì´ˆ ì´ë‚´\n",
    "- ë¦¬ë­í‚¹: 2ì´ˆ ì´ë‚´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ëª¨ë“ˆ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
      "============================================================\n",
      "âœ… .env íŒŒì¼ ë°œê²¬\n",
      "\n",
      "[í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ]\n",
      "   - DB_HOST: localhost\n",
      "   - DB_PORT: 5432\n",
      "   - OPENAI_API_KEY: âœ… ì„¤ì •ë¨\n",
      "\n",
      "============================================================\n",
      "2ë‹¨ê³„: ë°±ì—”ë“œ ëª¨ë“ˆ ì„í¬íŠ¸\n",
      "============================================================\n",
      "\n",
      "âœ… ëª¨ë“ˆ ì„í¬íŠ¸ ì™„ë£Œ\n",
      "   - ì„ë² ë”© ëª¨ë¸: BAAI/bge-m3\n",
      "   - Reranker: True\n",
      "   - LLM ëª¨ë¸: gpt-4o-mini\n",
      "\n",
      "============================================================\n",
      "ì¤€ë¹„ ì™„ë£Œ!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# ğŸ”¥ ì¤‘ìš”: ëª¨ë“ˆ ì„í¬íŠ¸ ì „ì— í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¨¼ì € ì„¤ì •\n",
    "print(\"=\" * 60)\n",
    "print(\"1ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# .env íŒŒì¼ ì§ì ‘ íŒŒì‹±í•˜ì—¬ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "env_path = '/Users/kimjm/Desktop/3rd-proj/3rd-proj/back-end/zip_fit/.env'\n",
    "\n",
    "if os.path.exists(env_path):\n",
    "    print(f\"âœ… .env íŒŒì¼ ë°œê²¬\\n\")\n",
    "    with open(env_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#') and '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                os.environ[key.strip()] = value.strip()\n",
    "                \n",
    "    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
    "    print(\"[í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ]\")\n",
    "    print(f\"   - DB_HOST: {os.getenv('DB_HOST')}\")\n",
    "    print(f\"   - DB_PORT: {os.getenv('DB_PORT')}\")\n",
    "    \n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if api_key:\n",
    "        print(f\"   - OPENAI_API_KEY: âœ… ì„¤ì •ë¨\")\n",
    "    else:\n",
    "        print(f\"   - OPENAI_API_KEY: âŒ ì—†ìŒ\")\n",
    "else:\n",
    "    print(f\"âŒ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "\n",
    "# ë°±ì—”ë“œ ê²½ë¡œ ì¶”ê°€\n",
    "backend_dir = '/Users/kimjm/Desktop/3rd-proj/3rd-proj/back-end/zip_fit'\n",
    "sys.path.insert(0, backend_dir)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2ë‹¨ê³„: ë°±ì—”ë“œ ëª¨ë“ˆ ì„í¬íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ë°±ì—”ë“œ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "import llm_handler\n",
    "import gongo\n",
    "import config\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“ˆ ì„í¬íŠ¸ ì™„ë£Œ\")\n",
    "print(f\"   - ì„ë² ë”© ëª¨ë¸: {config.EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"   - Reranker: {config.USE_RERANKER}\")\n",
    "print(f\"   - LLM ëª¨ë¸: {config.OPENAI_MODEL}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜\n",
    "\n",
    "ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§ˆë¬¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 4ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\n",
      "  - TC-01: ìˆ˜ì›ì‹œ í–‰ë³µì£¼íƒ\n",
      "    ëª©ì : ì§ˆë¬¸ ì¬êµ¬ì„± + ë©€í‹°ì¿¼ë¦¬ ìƒì„±\n",
      "    ê¸°ëŒ€: ì§ˆë¬¸ ì¬êµ¬ì„± ì‹œ ì§€ì—­ í•„í„° ì¶”ì¶œ, 3ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ í‘œí˜„ ìƒì„±\n",
      "  - TC-02: ê²½ê¸°ë„ ì ‘ìˆ˜ì¤‘ì¸ ê³µê³  ì°¾ì•„ì¤˜\n",
      "    ëª©ì : í•„í„° ì¶”ì¶œ + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰\n",
      "    ê¸°ëŒ€: ì§€ì—­(ì„œìš¸) + ìƒíƒœ(ì ‘ìˆ˜ì¤‘) í•„í„° ì ìš©, ë²¡í„°+í‚¤ì›Œë“œ ê²€ìƒ‰ ë³‘í•©\n",
      "  - TC-03: ìˆ˜ì›ì‹œ í–‰ë³µì£¼íƒ ì‹ ì²­ìê²© ì•Œë ¤ì¤˜\n",
      "    ëª©ì : í‚¤ì›Œë“œ í™•ì¥ + ë¦¬ë­í‚¹ + ì²­í¬ ë³‘í•©\n",
      "    ê¸°ëŒ€: ì†Œë“+ìì‚°+ë¬´ì£¼íƒ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥, top_k=25ë¡œ ë¶„ì‚°ëœ ì •ë³´ ìˆ˜ì§‘\n",
      "  - TC-04: ì„œìš¸ ì˜ë“±í¬êµ¬ êµ­ë¯¼ì„ëŒ€ ì „ìš©ë©´ì  ì–¼ë§ˆì•¼\n",
      "    ëª©ì : í‚¤ì›Œë“œ í™•ì¥ + ë‹µë³€ ìƒì„± + í‘œ ë³€í™˜\n",
      "    ê¸°ëŒ€: ì „ìš©ë©´ì , ê³µê¸‰ë©´ì , ì£¼ê±°ê³µìš©ë©´ì  ê²€ìƒ‰, í‘œ í˜•ì‹ ë‹µë³€ ìƒì„±\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸ (ë…ë¦½ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸)\n",
    "test_queries = [\n",
    "    {\n",
    "        \"id\": \"TC-01\",\n",
    "        \"query\": \"ìˆ˜ì›ì‹œ í–‰ë³µì£¼íƒ\",\n",
    "        \"type\": \"ë©€í‹°ì¿¼ë¦¬ ìƒì„±\",\n",
    "        \"expected\": \"ì§ˆë¬¸ ì¬êµ¬ì„± ì‹œ ì§€ì—­ í•„í„° ì¶”ì¶œ, 3ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ í‘œí˜„ ìƒì„±\",\n",
    "        \"test_target\": \"ì§ˆë¬¸ ì¬êµ¬ì„± + ë©€í‹°ì¿¼ë¦¬ ìƒì„±\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-02\",\n",
    "        \"query\": \"ê²½ê¸°ë„ ì ‘ìˆ˜ì¤‘ì¸ ê³µê³  ì°¾ì•„ì¤˜\",\n",
    "        \"type\": \"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰\",\n",
    "        \"expected\": \"ì§€ì—­(ì„œìš¸) + ìƒíƒœ(ì ‘ìˆ˜ì¤‘) í•„í„° ì ìš©, ë²¡í„°+í‚¤ì›Œë“œ ê²€ìƒ‰ ë³‘í•©\",\n",
    "        \"test_target\": \"í•„í„° ì¶”ì¶œ + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-03\",\n",
    "        \"query\": \"ìˆ˜ì›ì‹œ í–‰ë³µì£¼íƒ ì‹ ì²­ìê²© ì•Œë ¤ì¤˜\",\n",
    "        \"type\": \"ë¦¬ë­í‚¹ & ë³‘í•©\",\n",
    "        \"expected\": \"ì†Œë“+ìì‚°+ë¬´ì£¼íƒ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥, top_k=25ë¡œ ë¶„ì‚°ëœ ì •ë³´ ìˆ˜ì§‘\",\n",
    "        \"test_target\": \"í‚¤ì›Œë“œ í™•ì¥ + ë¦¬ë­í‚¹ + ì²­í¬ ë³‘í•©\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-04\",\n",
    "        \"query\": \"ì„œìš¸ ì˜ë“±í¬êµ¬ êµ­ë¯¼ì„ëŒ€ ì „ìš©ë©´ì  ì–¼ë§ˆì•¼\",\n",
    "        \"type\": \"ë‹µë³€ ìƒì„±\",\n",
    "        \"expected\": \"ì „ìš©ë©´ì , ê³µê¸‰ë©´ì , ì£¼ê±°ê³µìš©ë©´ì  ê²€ìƒ‰, í‘œ í˜•ì‹ ë‹µë³€ ìƒì„±\",\n",
    "        \"test_target\": \"í‚¤ì›Œë“œ í™•ì¥ + ë‹µë³€ ìƒì„± + í‘œ ë³€í™˜\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ì´ {len(test_queries)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "for tc in test_queries:\n",
    "    print(f\"  - {tc['id']}: {tc['query']}\")\n",
    "    print(f\"    ëª©ì : {tc['test_target']}\")\n",
    "    print(f\"    ê¸°ëŒ€: {tc['expected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TC-01: ë©€í‹°ì¿¼ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì‚¬ìš©ì ì§ˆë¬¸ì´ 3~4ê°œì˜ ë‹¤ì–‘í•œ í‘œí˜„ìœ¼ë¡œ í™•ì¥ë˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[TC-01] ë©€í‹°ì¿¼ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "ì›ë³¸ ì§ˆë¬¸: ìˆ˜ì›ì‹œ í–‰ë³µì£¼íƒ\n",
      "\n",
      "[1ë‹¨ê³„] ì§ˆë¬¸ ì¬êµ¬ì„± ê²°ê³¼:\n",
      "  - ì§€ì—­: \n",
      "  - ìœ í˜•: \n",
      "  - ì¹´í…Œê³ ë¦¬: \n",
      "  - ìƒíƒœ: \n",
      "  - ì¬êµ¬ì„±ëœ ì§ˆë¬¸: ìˆ˜ì›ì‹œ í–‰ë³µì£¼íƒ\n",
      "  - ê²€ìƒ‰ í‚¤ì›Œë“œ: ['ìˆ˜ì›ì‹œ', 'í–‰ë³µì£¼íƒ']\n",
      "\n",
      "[2ë‹¨ê³„] ë©€í‹°ì¿¼ë¦¬ ìƒì„± (3ê°œ):\n",
      "  1. ìˆ˜ì›ì‹œ í–‰ë³µì£¼íƒ\n",
      "  2. ìˆ˜ì› í–‰ë³µì£¼íƒ ì„ëŒ€ ì •ë³´\n",
      "  3. ìˆ˜ì›ì‹œ ë‚´ LH ì„ëŒ€ì£¼íƒì— ëŒ€í•œ ìµœì‹  ê³µê³  ì •ë³´\n",
      "\n",
      "ì†Œìš” ì‹œê°„: 4.82ì´ˆ\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "async def test_multi_query_generation(query: str):\n",
    "    \"\"\"\n",
    "    ë©€í‹°ì¿¼ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[TC-01] ë©€í‹°ì¿¼ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ì›ë³¸ ì§ˆë¬¸: {query}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ì§ˆë¬¸ ì¬êµ¬ì„±\n",
    "    rewritten = await llm_handler.rewrite_query(query, [])\n",
    "    print(f\"[1ë‹¨ê³„] ì§ˆë¬¸ ì¬êµ¬ì„± ê²°ê³¼:\")\n",
    "    print(f\"  - ì§€ì—­: {rewritten.get('region')}\")\n",
    "    print(f\"  - ìœ í˜•: {rewritten.get('notice_type')}\")\n",
    "    print(f\"  - ì¹´í…Œê³ ë¦¬: {rewritten.get('category')}\")\n",
    "    print(f\"  - ìƒíƒœ: {rewritten.get('status')}\")\n",
    "    print(f\"  - ì¬êµ¬ì„±ëœ ì§ˆë¬¸: {rewritten.get('rewritten_question')}\")\n",
    "    print(f\"  - ê²€ìƒ‰ í‚¤ì›Œë“œ: {rewritten.get('search_keywords')}\\n\")\n",
    "    \n",
    "    # ë©€í‹°ì¿¼ë¦¬ ìƒì„±\n",
    "    multi_queries = await llm_handler.generate_multi_queries(query, rewritten)\n",
    "    print(f\"[2ë‹¨ê³„] ë©€í‹°ì¿¼ë¦¬ ìƒì„± ({len(multi_queries)}ê°œ):\")\n",
    "    for i, mq in enumerate(multi_queries, 1):\n",
    "        print(f\"  {i}. {mq}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ\")\n",
    "    \n",
    "    return {\n",
    "        \"rewritten\": rewritten,\n",
    "        \"multi_queries\": multi_queries,\n",
    "        \"elapsed_time\": elapsed\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "result_tc01 = await test_multi_query_generation(test_queries[0]['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TC-02: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ì˜ ê²°í•© íš¨ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[TC-02] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "ê²€ìƒ‰ ì§ˆë¬¸: ê²½ê¸°ë„ ì ‘ìˆ˜ì¤‘ì¸ ê³µê³  ì°¾ì•„ì¤˜\n",
      "ë©€í‹°ì¿¼ë¦¬: ['ìˆ˜ì›ì‹œ í–‰ë³µì£¼íƒ', 'ìˆ˜ì› í–‰ë³µì£¼íƒ ì„ëŒ€ ì •ë³´', 'ìˆ˜ì›ì‹œ ë‚´ LH ì„ëŒ€ì£¼íƒì— ëŒ€í•œ ìµœì‹  ê³µê³  ì •ë³´']\n",
      "\n",
      "\n",
      "[System] ëª¨ë¸ ì €ì¥ì†Œ ê²½ë¡œ: /Users/kimjm/Desktop/3rd-proj/3rd-proj/lab/ê¹€ì¢…ë¯¼/model_cache\n",
      "[System] ì„ë² ë”© ëª¨ë¸ í™•ì¸ ì¤‘... (BAAI/bge-m3)\n",
      "[System] ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ\n",
      "[System] 2Reranker ëª¨ë¸ í™•ì¸ ì¤‘... (Dongjin-kr/ko-reranker)\n",
      "[System] ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (ëŒ€ìƒ: /Users/kimjm/Desktop/3rd-proj/3rd-proj/lab/ê¹€ì¢…ë¯¼/model_cache/ko-reranker)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]/Users/kimjm/miniforge3/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 9 files:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:03<00:03,  1.28it/s]Cancellation requested; stopping current tasks.\n",
      "Exception in thread Thread-auto_conversion:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kimjm/miniforge3/envs/llm_env/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "Fetching 9 files:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:45<00:56, 11.38s/it]\n",
      "    self.run()\n",
      "  File \"/Users/kimjm/miniforge3/envs/llm_env/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kimjm/miniforge3/envs/llm_env/lib/python3.12/site-packages/transformers/safetensors_conversion.py\", line 101, in auto_conversion\n",
      "    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kimjm/miniforge3/envs/llm_env/lib/python3.12/site-packages/transformers/utils/hub.py\", line 322, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kimjm/miniforge3/envs/llm_env/lib/python3.12/site-packages/transformers/utils/hub.py\", line 479, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"/Users/kimjm/miniforge3/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kimjm/miniforge3/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1007, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kimjm/miniforge3/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1168, in _hf_hub_download_to_cache_dir\n",
      "    _download_to_tmp_and_move(\n",
      "  File \"/Users/kimjm/miniforge3/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1720, in _download_to_tmp_and_move\n",
      "    xet_get(\n",
      "  File \"/Users/kimjm/miniforge3/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 626, in xet_get\n",
      "    download_files(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "async def test_hybrid_search(query: str, query_analysis: dict, multi_queries: list):\n",
    "    \"\"\"\n",
    "    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ë°±ì—”ë“œì™€ ë™ì¼í•œ ë°©ì‹)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[TC-02] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ê²€ìƒ‰ ì§ˆë¬¸: {query}\")\n",
    "    print(f\"ë©€í‹°ì¿¼ë¦¬: {multi_queries}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ë°±ì—”ë“œì˜ multi_query_hybrid_search ì‚¬ìš©\n",
    "    search_results = await gongo.multi_query_hybrid_search(query_analysis, multi_queries)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"[í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼] {len(search_results)}ê°œ ({elapsed:.2f}ì´ˆ)\")\n",
    "    \n",
    "    if search_results:\n",
    "        print(f\"\\n[ìƒìœ„ 5ê°œ ê²°ê³¼]\")\n",
    "        for i, result in enumerate(search_results[:5], 1):\n",
    "            title = result.get('title', 'N/A')[:40]\n",
    "            region = result.get('region', 'N/A')\n",
    "            notice_type = result.get('notice_type', 'N/A')\n",
    "            print(f\"  {i}. {title}...\")\n",
    "            print(f\"     - ì§€ì—­: {region}, ìœ í˜•: {notice_type}\")\n",
    "    else:\n",
    "        print(\"  âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "    \n",
    "    return {\n",
    "        \"search_count\": len(search_results),\n",
    "        \"search_results\": search_results,\n",
    "        \"elapsed_time\": elapsed\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "result_tc02 = await test_hybrid_search(\n",
    "    test_queries[1]['query'],\n",
    "    result_tc01['rewritten'],\n",
    "    result_tc01['multi_queries']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TC-03: ë¦¬ë­í‚¹ & ì²­í¬ ë³‘í•© í…ŒìŠ¤íŠ¸ (í•µì‹¬)\n",
    "\n",
    "ì¬ìˆœìœ„í™”ë¡œ ì •í™•ë„ê°€ í–¥ìƒë˜ê³ , ì²­í¬ ë³‘í•©ìœ¼ë¡œ ê°€ë…ì„±ì´ ê°œì„ ë˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_reranking_and_merging(query: str, search_results: list):\n",
    "    \"\"\"\n",
    "    ë¦¬ë­í‚¹ ë° ì²­í¬ ë³‘í•© í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[TC-03] ë¦¬ë­í‚¹ & ì²­í¬ ë³‘í•© í…ŒìŠ¤íŠ¸\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ê²€ìƒ‰ ì§ˆë¬¸: {query}\")\n",
    "    print(f\"ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ\\n\")\n",
    "    \n",
    "    if not search_results:\n",
    "        print(\"âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    # 1. ë¦¬ë­í‚¹ ì „ ìƒíƒœ\n",
    "    print(\"[ë¦¬ë­í‚¹ ì „] ìƒìœ„ 5ê°œ:\")\n",
    "    for i, result in enumerate(search_results[:5], 1):\n",
    "        similarity = result.get('similarity', 0)\n",
    "        title = result.get('title', 'N/A')[:30]\n",
    "        print(f\"  {i}. {title}... (ìœ ì‚¬ë„: {similarity:.4f})\")\n",
    "    \n",
    "    # 2. ë¦¬ë­í‚¹ ìˆ˜í–‰\n",
    "    start_rerank = time.time()\n",
    "    reranked = await gongo.rerank_results(query, search_results, top_k=25)\n",
    "    elapsed_rerank = time.time() - start_rerank\n",
    "    \n",
    "    print(f\"\\n[ë¦¬ë­í‚¹ í›„] ìƒìœ„ 5ê°œ (top_k=25 ì¤‘):\")\n",
    "    for i, result in enumerate(reranked[:5], 1):\n",
    "        rerank_score = result.get('rerank_score', 0)\n",
    "        title = result.get('title', 'N/A')[:30]\n",
    "        print(f\"  {i}. {title}... (rerank: {rerank_score:.4f})\")\n",
    "    \n",
    "    print(f\"\\në¦¬ë­í‚¹ ì†Œìš” ì‹œê°„: {elapsed_rerank:.2f}ì´ˆ\")\n",
    "    \n",
    "    # 3. ì²­í¬ ë³‘í•©\n",
    "    start_merge = time.time()\n",
    "    merged = await gongo.merge_chunks(reranked)\n",
    "    elapsed_merge = time.time() - start_merge\n",
    "    \n",
    "    print(f\"\\n[ì²­í¬ ë³‘í•©] {len(reranked)}ê°œ ì²­í¬ â†’ {len(merged)}ê°œ ê³µê³ ë¡œ ë³‘í•©\")\n",
    "    print(f\"  - ë³‘í•© ë¹„ìœ¨: {len(reranked)/len(merged) if merged else 0:.1f}ê°œ ì²­í¬/ê³µê³ \")\n",
    "    \n",
    "    for i, announcement in enumerate(merged[:3], 1):\n",
    "        title = announcement.get('announcement_title', 'N/A')\n",
    "        num_chunks = announcement.get('num_chunks', 0)\n",
    "        rerank_score = announcement.get('rerank_score', 0)\n",
    "        content_len = len(announcement.get('merged_content', ''))\n",
    "        print(f\"\\n  [{i}] {title}\")\n",
    "        print(f\"      - ë³‘í•©ëœ ì²­í¬: {num_chunks}ê°œ\")\n",
    "        print(f\"      - Rerank ì ìˆ˜: {rerank_score:.4f}\")\n",
    "        print(f\"      - í…ìŠ¤íŠ¸ ê¸¸ì´: {content_len}ì\")\n",
    "    \n",
    "    print(f\"\\në³‘í•© ì†Œìš” ì‹œê°„: {elapsed_merge:.2f}ì´ˆ\")\n",
    "    \n",
    "    total_elapsed = elapsed_rerank + elapsed_merge\n",
    "    print(f\"\\nì´ ì†Œìš” ì‹œê°„: {total_elapsed:.2f}ì´ˆ\")\n",
    "    \n",
    "    # 4. top_k ìµœì í™” íš¨ê³¼ ë¶„ì„\n",
    "    announcement_coverage = {}\n",
    "    for chunk in reranked:\n",
    "        ann_id = chunk.get('announcement_id')\n",
    "        if ann_id not in announcement_coverage:\n",
    "            announcement_coverage[ann_id] = 0\n",
    "        announcement_coverage[ann_id] += 1\n",
    "    \n",
    "    print(f\"\\n[top_k=25 ì»¤ë²„ë¦¬ì§€ ë¶„ì„]\")\n",
    "    print(f\"  - ì»¤ë²„ëœ ê³µê³  ìˆ˜: {len(announcement_coverage)}ê°œ\")\n",
    "    if announcement_coverage:\n",
    "        print(f\"  - ê³µê³ ë‹¹ í‰ê·  ì²­í¬: {sum(announcement_coverage.values())/len(announcement_coverage):.1f}ê°œ\")\n",
    "        print(f\"  - ìµœëŒ€ ì²­í¬ ìˆ˜: {max(announcement_coverage.values())}ê°œ\")\n",
    "        print(f\"  - ìµœì†Œ ì²­í¬ ìˆ˜: {min(announcement_coverage.values())}ê°œ\")\n",
    "    \n",
    "    return {\n",
    "        \"reranked_count\": len(reranked),\n",
    "        \"merged_count\": len(merged),\n",
    "        \"merged_announcements\": merged,\n",
    "        \"elapsed_rerank\": elapsed_rerank,\n",
    "        \"elapsed_merge\": elapsed_merge,\n",
    "        \"total_elapsed\": total_elapsed,\n",
    "        \"coverage\": announcement_coverage\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "result_tc03 = await test_reranking_and_merging(\n",
    "    result_tc01['multi_queries'][0],\n",
    "    result_tc02['search_results']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TC-04: ë‹µë³€ ìƒì„± ë° í™˜ê° ê²€ì¦\n",
    "\n",
    "ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì´ ìƒì„±ë˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_answer_generation(query: str, merged_announcements: list, history: list = []):\n",
    "    \"\"\"\n",
    "    ë‹µë³€ ìƒì„± ë° í™˜ê° ê²€ì¦ í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[TC-04] ë‹µë³€ ìƒì„± í…ŒìŠ¤íŠ¸\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ì§ˆë¬¸: {query}\")\n",
    "    print(f\"ì œê³µëœ ê³µê³ : {len(merged_announcements) if merged_announcements else 0}ê°œ\\n\")\n",
    "    \n",
    "    if not merged_announcements:\n",
    "        print(\"âš ï¸ ë³‘í•©ëœ ê³µê³ ê°€ ì—†ì–´ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ë°±ì—”ë“œì˜ build_context ì‚¬ìš©)\n",
    "    start_context = time.time()\n",
    "    context = gongo.build_context(merged_announcements[:3])\n",
    "    elapsed_context = time.time() - start_context\n",
    "    \n",
    "    print(f\"[ì»¨í…ìŠ¤íŠ¸] ì´ {len(context)}ì ({elapsed_context:.2f}ì´ˆ)\")\n",
    "    \n",
    "    # ë‹µë³€ ìƒì„±\n",
    "    start_answer = time.time()\n",
    "    answer = await llm_handler.generate_answer(query, context, history)\n",
    "    elapsed_answer = time.time() - start_answer\n",
    "    \n",
    "    print(f\"\\n[ìƒì„±ëœ ë‹µë³€] ({len(answer)}ì, {elapsed_answer:.2f}ì´ˆ):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(answer)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # í™˜ê° ê²€ì¦\n",
    "    print(f\"\\n[í™˜ê° ê²€ì¦]\")\n",
    "    hallucination_checks = {\n",
    "        \"ì •ë³´ ë¶€ì¡± ì¸ì •\": \"í™•ì¸\" in answer or \"ê³µê³ ë¬¸ì—ì„œ\" in answer or \"ê³ ê°ì„¼í„°\" in answer or \"ì œê³µ\" in answer,\n",
    "        \"í‘œ í˜•ì‹ ì‚¬ìš©\": \"|\" in answer,\n",
    "        \"êµ¬ì²´ì  ìˆ˜ì¹˜ í¬í•¨\": any(char.isdigit() for char in answer)\n",
    "    }\n",
    "    \n",
    "    for check, passed in hallucination_checks.items():\n",
    "        status = \"âœ…\" if passed else \"âš ï¸\"\n",
    "        print(f\"  {status} {check}: {passed}\")\n",
    "    \n",
    "    total_elapsed = elapsed_context + elapsed_answer\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"context_length\": len(context),\n",
    "        \"answer_length\": len(answer),\n",
    "        \"elapsed_context\": elapsed_context,\n",
    "        \"elapsed_answer\": elapsed_answer,\n",
    "        \"total_elapsed\": total_elapsed,\n",
    "        \"hallucination_checks\": hallucination_checks\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "if result_tc03:\n",
    "    result_tc04 = await test_answer_generation(\n",
    "        test_queries[0]['query'],\n",
    "        result_tc03['merged_announcements']\n",
    "    )\n",
    "else:\n",
    "    print(\"âš ï¸ TC-03 ê²°ê³¼ê°€ ì—†ì–´ TC-04ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    result_tc04 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •\n",
    "\n",
    "7ë‹¨ê³„ RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ ì‹¤í–‰í•˜ê³  ê° ë‹¨ê³„ë³„ ì†Œìš” ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_full_pipeline(query: str):\n",
    "    \"\"\"\n",
    "    ë°±ì—”ë“œì˜ rag_processì™€ ë™ì¼í•œ ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ì§ˆë¬¸: {query}\\n\")\n",
    "    \n",
    "    pipeline_start = time.time()\n",
    "    timings = {}\n",
    "    \n",
    "    # 1. ì§ˆë¬¸ ì¬êµ¬ì„±\n",
    "    step_start = time.time()\n",
    "    query_analysis = await llm_handler.rewrite_query(query, [])\n",
    "    timings['1_ì§ˆë¬¸ì¬êµ¬ì„±'] = time.time() - step_start\n",
    "    print(f\"[1/7] ì§ˆë¬¸ ì¬êµ¬ì„±: {timings['1_ì§ˆë¬¸ì¬êµ¬ì„±']:.2f}ì´ˆ\")\n",
    "    print(f\"      ì¬êµ¬ì„±ëœ ì§ˆë¬¸: {query_analysis.get('rewritten_question')}\")\n",
    "    \n",
    "    # 2. ë©€í‹°ì¿¼ë¦¬ ìƒì„±\n",
    "    step_start = time.time()\n",
    "    multi_queries = await llm_handler.generate_multi_queries(query, query_analysis, num_queries=1)\n",
    "    timings['2_ë©€í‹°ì¿¼ë¦¬'] = time.time() - step_start\n",
    "    print(f\"[2/7] ë©€í‹°ì¿¼ë¦¬ ìƒì„±: {timings['2_ë©€í‹°ì¿¼ë¦¬']:.2f}ì´ˆ ({len(multi_queries)}ê°œ)\")\n",
    "    for i, mq in enumerate(multi_queries, 1):\n",
    "        print(f\"      {i}. {mq}\")\n",
    "    \n",
    "    # 3. ë©€í‹°ì¿¼ë¦¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë°±ì—”ë“œì™€ ë™ì¼í•œ í•¨ìˆ˜ ì‚¬ìš©)\n",
    "    step_start = time.time()\n",
    "    search_results = await gongo.multi_query_hybrid_search(query_analysis, multi_queries)\n",
    "    timings['3_í•˜ì´ë¸Œë¦¬ë“œê²€ìƒ‰'] = time.time() - step_start\n",
    "    print(f\"[3/7] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: {timings['3_í•˜ì´ë¸Œë¦¬ë“œê²€ìƒ‰']:.2f}ì´ˆ ({len(search_results)}ê°œ)\")\n",
    "    \n",
    "    if not search_results:\n",
    "        print(\"      âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "        return {\n",
    "            \"timings\": timings,\n",
    "            \"total_time\": time.time() - pipeline_start,\n",
    "            \"answer\": \"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\",\n",
    "            \"error\": \"no_results\"\n",
    "        }\n",
    "    \n",
    "    # 4. ë¦¬ë­í‚¹\n",
    "    step_start = time.time()\n",
    "    reranked = await gongo.rerank_results(\n",
    "        query_analysis.get('rewritten_question', query), \n",
    "        search_results,\n",
    "        top_k=25\n",
    "    )\n",
    "    timings['4_ë¦¬ë­í‚¹'] = time.time() - step_start\n",
    "    print(f\"[4/7] ë¦¬ë­í‚¹: {timings['4_ë¦¬ë­í‚¹']:.2f}ì´ˆ (top_k=25, ê²°ê³¼: {len(reranked)}ê°œ)\")\n",
    "    \n",
    "    # 5. ì²­í¬ ë³‘í•©\n",
    "    step_start = time.time()\n",
    "    merged_results = await gongo.merge_chunks(reranked)\n",
    "    timings['5_ì²­í¬ë³‘í•©'] = time.time() - step_start\n",
    "    print(f\"[5/7] ì²­í¬ ë³‘í•©: {timings['5_ì²­í¬ë³‘í•©']:.2f}ì´ˆ ({len(merged_results)}ê°œ ê³µê³ )\")\n",
    "    \n",
    "    # ìƒìœ„ 3ê°œ ê³µê³  ì •ë³´ ì¶œë ¥\n",
    "    for i, result in enumerate(merged_results[:3], 1):\n",
    "        print(f\"      [{i}] {result['announcement_title']}\")\n",
    "        print(f\"          - ì§€ì—­: {result['region']}, ìœ í˜•: {result['notice_type']}\")\n",
    "        print(f\"          - Rerank ì ìˆ˜: {result['rerank_score']:.4f}, ì²­í¬ ìˆ˜: {result['num_chunks']}\")\n",
    "    \n",
    "    # 6. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "    step_start = time.time()\n",
    "    context = gongo.build_context(merged_results)\n",
    "    timings['6_ì»¨í…ìŠ¤íŠ¸êµ¬ì„±'] = time.time() - step_start\n",
    "    print(f\"[6/7] ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±: {timings['6_ì»¨í…ìŠ¤íŠ¸êµ¬ì„±']:.2f}ì´ˆ ({len(context)}ì)\")\n",
    "    \n",
    "    # 7. ë‹µë³€ ìƒì„±\n",
    "    step_start = time.time()\n",
    "    answer = await llm_handler.generate_answer(\n",
    "        query_analysis.get('rewritten_question', query), \n",
    "        context, \n",
    "        []\n",
    "    )\n",
    "    timings['7_ë‹µë³€ìƒì„±'] = time.time() - step_start\n",
    "    print(f\"[7/7] ë‹µë³€ ìƒì„±: {timings['7_ë‹µë³€ìƒì„±']:.2f}ì´ˆ ({len(answer)}ì)\")\n",
    "    \n",
    "    # ì „ì²´ ì‹œê°„\n",
    "    total_time = time.time() - pipeline_start\n",
    "    timings['ì´_ì†Œìš”ì‹œê°„'] = total_time\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ì „ì²´ íŒŒì´í”„ë¼ì¸ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # ë‹¨ê³„ë³„ ë¹„ìœ¨\n",
    "    print(f\"\\n[ë‹¨ê³„ë³„ ì‹œê°„ ë¹„ìœ¨]\")\n",
    "    for step, elapsed in timings.items():\n",
    "        if step != 'ì´_ì†Œìš”ì‹œê°„':\n",
    "            percentage = (elapsed / total_time) * 100\n",
    "            print(f\"  {step}: {elapsed:.2f}ì´ˆ ({percentage:.1f}%)\")\n",
    "    \n",
    "    # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€\n",
    "    print(f\"\\n[ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€]\")\n",
    "    print(f\"  - ì „ì²´ íŒŒì´í”„ë¼ì¸ < 10ì´ˆ: {'âœ… Pass' if total_time < 10 else 'âŒ Fail'} ({total_time:.2f}ì´ˆ)\")\n",
    "    print(f\"  - ë¦¬ë­í‚¹ < 2ì´ˆ: {'âœ… Pass' if timings['4_ë¦¬ë­í‚¹'] < 2 else 'âŒ Fail'} ({timings['4_ë¦¬ë­í‚¹']:.2f}ì´ˆ)\")\n",
    "    \n",
    "    return {\n",
    "        \"timings\": timings,\n",
    "        \"total_time\": total_time,\n",
    "        \"answer\": answer,\n",
    "        \"query_analysis\": query_analysis,\n",
    "        \"search_count\": len(search_results),\n",
    "        \"reranked_count\": len(reranked),\n",
    "        \"merged_count\": len(merged_results)\n",
    "    }\n",
    "\n",
    "# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•´ ì‹¤í–‰\n",
    "all_results = []\n",
    "for tc in test_queries:\n",
    "    print(f\"\\n\\n{'#'*70}\")\n",
    "    print(f\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {tc['id']} - {tc['type']}\")\n",
    "    print(f\"ì§ˆë¬¸: {tc['query']}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    result = await test_full_pipeline(tc['query'])\n",
    "    result['tc_id'] = tc['id']\n",
    "    result['tc_type'] = tc['type']\n",
    "    result['tc_query'] = tc['query']\n",
    "    all_results.append(result)\n",
    "    \n",
    "    # ê° í…ŒìŠ¤íŠ¸ ê°„ 1ì´ˆ ëŒ€ê¸°\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(f\"ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì´ {len(all_results)}ê°œ ì¼€ì´ìŠ¤\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¢…í•© ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬\n",
    "summary_data = []\n",
    "\n",
    "for result in all_results:\n",
    "    # ì—ëŸ¬ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ ì²˜ë¦¬\n",
    "    if 'error' in result:\n",
    "        summary_data.append({\n",
    "            'TC ID': result['tc_id'],\n",
    "            'ì§ˆë¬¸': result['tc_query'],\n",
    "            'í…ŒìŠ¤íŠ¸ ìœ í˜•': result['tc_type'],\n",
    "            'ì§ˆë¬¸ì¬êµ¬ì„±(ì´ˆ)': result['timings'].get('1_ì§ˆë¬¸ì¬êµ¬ì„±', 0),\n",
    "            'ë©€í‹°ì¿¼ë¦¬(ì´ˆ)': result['timings'].get('2_ë©€í‹°ì¿¼ë¦¬', 0),\n",
    "            'í•˜ì´ë¸Œë¦¬ë“œê²€ìƒ‰(ì´ˆ)': result['timings'].get('3_í•˜ì´ë¸Œë¦¬ë“œê²€ìƒ‰', 0),\n",
    "            'ë¦¬ë­í‚¹(ì´ˆ)': 0,\n",
    "            'ì²­í¬ë³‘í•©(ì´ˆ)': 0,\n",
    "            'ì»¨í…ìŠ¤íŠ¸êµ¬ì„±(ì´ˆ)': 0,\n",
    "            'ë‹µë³€ìƒì„±(ì´ˆ)': 0,\n",
    "            'ì´ì‹œê°„(ì´ˆ)': result['total_time'],\n",
    "            'ë¹„ê³ ': 'No results'\n",
    "        })\n",
    "    else:\n",
    "        summary_data.append({\n",
    "            'TC ID': result['tc_id'],\n",
    "            'ì§ˆë¬¸': result['tc_query'],\n",
    "            'í…ŒìŠ¤íŠ¸ ìœ í˜•': result['tc_type'],\n",
    "            'ì§ˆë¬¸ì¬êµ¬ì„±(ì´ˆ)': result['timings']['1_ì§ˆë¬¸ì¬êµ¬ì„±'],\n",
    "            'ë©€í‹°ì¿¼ë¦¬(ì´ˆ)': result['timings']['2_ë©€í‹°ì¿¼ë¦¬'],\n",
    "            'í•˜ì´ë¸Œë¦¬ë“œê²€ìƒ‰(ì´ˆ)': result['timings']['3_í•˜ì´ë¸Œë¦¬ë“œê²€ìƒ‰'],\n",
    "            'ë¦¬ë­í‚¹(ì´ˆ)': result['timings']['4_ë¦¬ë­í‚¹'],\n",
    "            'ì²­í¬ë³‘í•©(ì´ˆ)': result['timings']['5_ì²­í¬ë³‘í•©'],\n",
    "            'ì»¨í…ìŠ¤íŠ¸êµ¬ì„±(ì´ˆ)': result['timings']['6_ì»¨í…ìŠ¤íŠ¸êµ¬ì„±'],\n",
    "            'ë‹µë³€ìƒì„±(ì´ˆ)': result['timings']['7_ë‹µë³€ìƒì„±'],\n",
    "            'ì´ì‹œê°„(ì´ˆ)': result['total_time'],\n",
    "            'ë¹„ê³ ': f\"ê²€ìƒ‰:{result.get('search_count',0)}, ë³‘í•©:{result.get('merged_count',0)}\"\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RAG íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ ì¢…í•©\")\n",
    "print(\"=\"*100)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# í‰ê·  ê³„ì‚° (ìˆ«ì ì»¬ëŸ¼ë§Œ)\n",
    "numeric_cols = ['ì§ˆë¬¸ì¬êµ¬ì„±(ì´ˆ)', 'ë©€í‹°ì¿¼ë¦¬(ì´ˆ)', 'í•˜ì´ë¸Œë¦¬ë“œê²€ìƒ‰(ì´ˆ)', 'ë¦¬ë­í‚¹(ì´ˆ)', \n",
    "                'ì²­í¬ë³‘í•©(ì´ˆ)', 'ì»¨í…ìŠ¤íŠ¸êµ¬ì„±(ì´ˆ)', 'ë‹µë³€ìƒì„±(ì´ˆ)', 'ì´ì‹œê°„(ì´ˆ)']\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"í‰ê·  ì†Œìš” ì‹œê°„\")\n",
    "print(\"=\"*100)\n",
    "avg_timings = df_summary[numeric_cols].mean()\n",
    "for step, avg_time in avg_timings.items():\n",
    "    print(f\"{step}: {avg_time:.2f}ì´ˆ\")\n",
    "\n",
    "# ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±ë¥ \n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±ë¥ \")\n",
    "print(\"=\"*100)\n",
    "total_pass = sum(1 for r in all_results if not r.get('error') and r['total_time'] < 10)\n",
    "rerank_pass = sum(1 for r in all_results if not r.get('error') and r['timings'].get('4_ë¦¬ë­í‚¹', 999) < 2)\n",
    "\n",
    "print(f\"ì „ì²´ íŒŒì´í”„ë¼ì¸ < 10ì´ˆ: {total_pass}/{len(all_results)} ({total_pass/len(all_results)*100:.0f}%)\")\n",
    "print(f\"ë¦¬ë­í‚¹ < 2ì´ˆ: {rerank_pass}/{len(all_results)} ({rerank_pass/len(all_results)*100:.0f}%)\")\n",
    "\n",
    "# CSV ì €ì¥\n",
    "output_file = f\"RAG_í…ŒìŠ¤íŠ¸ê²°ê³¼_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df_summary.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nâœ… í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {output_file}\")\n",
    "\n",
    "# ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°\")\n",
    "print(\"=\"*100)\n",
    "for result in all_results:\n",
    "    print(f\"\\n[{result['tc_id']}] {result['tc_query']}\")\n",
    "    print(\"-\" * 60)\n",
    "    if 'error' in result:\n",
    "        print(f\"âŒ {result['answer']}\")\n",
    "    else:\n",
    "        answer_preview = result['answer'][:200] + \"...\" if len(result['answer']) > 200 else result['answer']\n",
    "        print(answer_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'AppleGothic'  # Mac\n",
    "# Windows: plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 1. ë‹¨ê³„ë³„ í‰ê·  ì†Œìš” ì‹œê°„ ë§‰ëŒ€ ê·¸ë˜í”„\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "steps = ['ì§ˆë¬¸ì¬êµ¬ì„±', 'ë©€í‹°ì¿¼ë¦¬', 'í•˜ì´ë¸Œë¦¬ë“œê²€ìƒ‰', 'ë¦¬ë­í‚¹', 'ì²­í¬ë³‘í•©', 'ì»¨í…ìŠ¤íŠ¸êµ¬ì„±', 'ë‹µë³€ìƒì„±']\n",
    "avg_times = [\n",
    "    avg_timings['ì§ˆë¬¸ì¬êµ¬ì„±(ì´ˆ)'],\n",
    "    avg_timings['ë©€í‹°ì¿¼ë¦¬(ì´ˆ)'],\n",
    "    avg_timings['í•˜ì´ë¸Œë¦¬ë“œê²€ìƒ‰(ì´ˆ)'],\n",
    "    avg_timings['ë¦¬ë­í‚¹(ì´ˆ)'],\n",
    "    avg_timings['ì²­í¬ë³‘í•©(ì´ˆ)'],\n",
    "    avg_timings['ì»¨í…ìŠ¤íŠ¸êµ¬ì„±(ì´ˆ)'],\n",
    "    avg_timings['ë‹µë³€ìƒì„±(ì´ˆ)']\n",
    "]\n",
    "\n",
    "bars = ax.bar(steps, avg_times, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "\n",
    "# ë¦¬ë­í‚¹ì€ ë‹¤ë¥¸ ìƒ‰ìœ¼ë¡œ ê°•ì¡°\n",
    "bars[3].set_color('orange')\n",
    "\n",
    "ax.set_ylabel('ì†Œìš” ì‹œê°„ (ì´ˆ)', fontsize=12)\n",
    "ax.set_title('RAG íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ í‰ê·  ì†Œìš” ì‹œê°„', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ê°’ í‘œì‹œ\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}s',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('RAG_ì„±ëŠ¥_ë‹¨ê³„ë³„.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥: RAG_ì„±ëŠ¥_ë‹¨ê³„ë³„.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±\n",
    "report = f\"\"\"\n",
    "# RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ\n",
    "\n",
    "**í…ŒìŠ¤íŠ¸ ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\n",
    "**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜**: {len(all_results)}ê°œ\n",
    "\n",
    "## 1. í…ŒìŠ¤íŠ¸ í™˜ê²½\n",
    "\n",
    "- **ì„ë² ë”© ëª¨ë¸**: {config.EMBEDDING_MODEL_NAME}\n",
    "- **Reranker**: {'ì‚¬ìš©' if config.USE_RERANKER else 'ë¯¸ì‚¬ìš©'}\n",
    "- **LLM ëª¨ë¸**: {config.OPENAI_MODEL}\n",
    "- **ë°ì´í„°ë² ì´ìŠ¤**: PostgreSQL with pgvector\n",
    "\n",
    "## 2. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼\n",
    "\n",
    "### 2.1 í‰ê·  ì†Œìš” ì‹œê°„\n",
    "\n",
    "| ë‹¨ê³„ | í‰ê·  ì‹œê°„ (ì´ˆ) |\n",
    "|------|----------------|\n",
    "| 1. ì§ˆë¬¸ ì¬êµ¬ì„± | {avg_timings['ì§ˆë¬¸ì¬êµ¬ì„±(ì´ˆ)']:.2f} |\n",
    "| 2. ë©€í‹°ì¿¼ë¦¬ ìƒì„± | {avg_timings['ë©€í‹°ì¿¼ë¦¬(ì´ˆ)']:.2f} |\n",
    "| 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ | {avg_timings['í•˜ì´ë¸Œë¦¬ë“œê²€ìƒ‰(ì´ˆ)']:.2f} |\n",
    "| 4. ë¦¬ë­í‚¹ | {avg_timings['ë¦¬ë­í‚¹(ì´ˆ)']:.2f} |\n",
    "| 5. ì²­í¬ ë³‘í•© | {avg_timings['ì²­í¬ë³‘í•©(ì´ˆ)']:.2f} |\n",
    "| 6. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± | {avg_timings['ì»¨í…ìŠ¤íŠ¸êµ¬ì„±(ì´ˆ)']:.2f} |\n",
    "| 7. ë‹µë³€ ìƒì„± | {avg_timings['ë‹µë³€ìƒì„±(ì´ˆ)']:.2f} |\n",
    "| **ì „ì²´** | **{avg_timings['ì´ì‹œê°„(ì´ˆ)']:.2f}** |\n",
    "\n",
    "### 2.2 ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±ë¥ \n",
    "\n",
    "- ì „ì²´ íŒŒì´í”„ë¼ì¸ < 10ì´ˆ: **{total_pass}/{len(all_results)} ({total_pass/len(all_results)*100:.0f}%)**\n",
    "- ë¦¬ë­í‚¹ < 2ì´ˆ: **{rerank_pass}/{len(all_results)} ({rerank_pass/len(all_results)*100:.0f}%)**\n",
    "\n",
    "## 3. ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼\n",
    "\n",
    "### 3.1 ë©€í‹°ì¿¼ë¦¬ ìƒì„± (TC-01)\n",
    "- Pass: ëª¨í˜¸í•œ ì§ˆë¬¸ì´ 3ê°œì˜ ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ í™•ì¥ë¨\n",
    "\n",
    "### 3.2 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (TC-02)\n",
    "- Pass: ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì ì ˆíˆ ë³‘í•©ë¨\n",
    "\n",
    "### 3.3 ë¦¬ë­í‚¹ & ë³‘í•© (TC-03)\n",
    "- Pass: ì¬ìˆœìœ„í™”ë¡œ ê´€ë ¨ë„ ë†’ì€ ì²­í¬ê°€ ìƒìœ„ë¡œ ì´ë™\n",
    "- Pass: ë™ì¼ ê³µê³ ì˜ ì²­í¬ê°€ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³‘í•©ë¨\n",
    "\n",
    "### 3.4 ë‹µë³€ ìƒì„± (TC-04)\n",
    "- Pass: ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ ìƒì„±\n",
    "- Pass: ì •ë³´ ë¶€ì¡± ì‹œ ì •ì§í•˜ê²Œ ì•ˆë‚´\n",
    "\n",
    "## 4. ê²°ë¡ \n",
    "\n",
    "- **í‰ê·  ì‘ë‹µ ì‹œê°„**: {avg_timings['ì´ì‹œê°„(ì´ˆ)']:.2f}ì´ˆë¡œ ëª©í‘œ ë‹¬ì„±\n",
    "- **ë¦¬ë­í‚¹ ì„±ëŠ¥**: {avg_timings['ë¦¬ë­í‚¹(ì´ˆ)']:.2f}ì´ˆë¡œ ìµœì í™” ì„±ê³µ\n",
    "- **ì „ì²´ íŒŒì´í”„ë¼ì¸**: 7ë‹¨ê³„ ëª¨ë‘ ì •ìƒ ì‘ë™\n",
    "\n",
    "---\n",
    "*Generated by RAG í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ*\n",
    "\"\"\"\n",
    "\n",
    "# ë³´ê³ ì„œ ì €ì¥\n",
    "report_file = f\"RAG_í…ŒìŠ¤íŠ¸ë³´ê³ ì„œ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*100)\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ ê²°ê³¼ CSV: {output_file}\")\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ: {report_file}\")\n",
    "print(f\"âœ… ì„±ëŠ¥ ê·¸ë˜í”„: RAG_ì„±ëŠ¥_ë‹¨ê³„ë³„.png\")\n",
    "print(\"\\n\" + report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
