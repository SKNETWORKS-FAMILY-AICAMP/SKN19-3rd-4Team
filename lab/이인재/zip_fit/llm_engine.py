from typing import Dict, Any
import asyncio
from .models import ChatRequest 
# ğŸŒŸ config.pyì—ì„œ ì„¤ì •ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from .config import settings
# ğŸŒŸ Gongo ì„í¬íŠ¸
from .gongo import Gongo
# ğŸŒŸ OpenAI ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
from openai import AsyncOpenAI

class LlmEngine:
    """
    LLM í˜¸ì¶œ, í”„ë¡¬í”„íŠ¸ êµ¬ì„±, LangChain/LangGraph ë“±ì˜ ì§€ëŠ¥í˜• ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    # ğŸŒŸ ìƒì„±ìë¥¼ í†µí•´ Gongo ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì£¼ì…ë°›ìŠµë‹ˆë‹¤.
    def __init__(self, gongo_service: Gongo):
        self.gongo_service = gongo_service
    # ğŸŒŸ ì‹¤ì œ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Configì—ì„œ API Key ì‚¬ìš©)
        self.openai_client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        print("âš™ï¸ LlmEngine Initialized with Gongo service.")

    # ----------------------------------------------------
    # ğŸŒŸ ìš”ì²­í•˜ì‹  ë©”ì„œë“œ 1: Gongoì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ì˜¤ëŠ” ë©”ì„œë“œ
    # ----------------------------------------------------
    async def _get_llm_input_text(self, request: ChatRequest) -> str:
        """
        Gongo ì„œë¹„ìŠ¤ì—ì„œ RAG ë° ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ LLM ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        # Gongo ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        context_data = await self.gongo_service.get_contextual_data(
            user_id=request.user_id, 
            query=request.user_input
        )
        
        # ìµœì¢…ì ìœ¼ë¡œ LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
        llm_input_text = (
            f"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n\n"
            f"{context_data}\n\n"
            f"ì‚¬ìš©ì ì§ˆë¬¸: {request.user_input}"
        )
        
        return llm_input_text

    # ----------------------------------------------------
    # ğŸŒŸ ìš”ì²­í•˜ì‹  ë©”ì„œë“œ 2: LLMì„ í˜¸ì¶œí•˜ëŠ” ë©”ì„œë“œ (Mock)
    # ----------------------------------------------------
    async def _call_llm_api(self, prompt_text: str) -> Dict[str, Any]:
        """
        OpenAI, LangChain ë“±ì„ ì´ìš©í•˜ì—¬ ì‹¤ì œ LLM APIë¥¼ í˜¸ì¶œí•˜ê³  ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤. (í˜„ì¬ëŠ” Mock)
        """
        
        # ğŸŒŸ ì‹¤ì œ OpenAI LLM í˜¸ì¶œ
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = "ë‹¹ì‹ ì€ ìµœê³ ì˜ ë¶„ì„ê°€ì´ì ì¡°ì–¸ìì…ë‹ˆë‹¤. ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”"
        
        # APIì— ì „ë‹¬í•  ë©”ì„¸ì§€
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt_text}
        ]
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.3
            )
            # ì‘ë‹µ ê²°ê³¼ íŒŒì‹±
            llm_output = response.choices[0].message.content
            
            return {
                "llm_output": llm_output,
                "prompt_used": prompt_text,
                "usage_tokens": response.usage.total_tokens
            }
        
        except Exception as e:
            # ì˜¤ë¥˜ ì²˜ë¦¬
            return {
                "llm_output": f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "prompt_used": prompt_text,
                "usage_tokens": 0
            }
        
        # # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´ ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤.
        # await asyncio.sleep(0.05)
        
        # # Mock ì‘ë‹µì„ êµ¬ì„±í•©ë‹ˆë‹¤.
        # mock_llm_response = {
        #     "llm_output": f"LLMì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt_text)} ë¬¸ì)",
        #     "prompt_used": prompt_text,
        #     "usage_tokens": len(prompt_text) // 5 # ëŒ€ëµì ì¸ í† í° Mock
        # }
        
        # return mock_llm_response


    async def generate_response(self, request: ChatRequest) -> Dict[str, Any]:
        """
        Chatting í´ë˜ìŠ¤ì—ì„œ í˜¸ì¶œë˜ëŠ” ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œì…ë‹ˆë‹¤.
        """
        # 1. Gongoë¥¼ í†µí•´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        prompt_text = await self._get_llm_input_text(request)
        
        # 2. LLM í˜¸ì¶œ
        llm_result = await self._call_llm_api(prompt_text)
        
        return llm_result