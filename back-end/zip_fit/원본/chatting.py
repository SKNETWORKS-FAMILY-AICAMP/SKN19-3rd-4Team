from typing import Dict, Any
# ðŸŒŸ models.pyì—ì„œ Pydantic ëª¨ë¸ ìž„í¬íŠ¸
from .models import ChatRequest, ChatResponse
# LlmEngine ìž„í¬íŠ¸
from .llm_engine import LlmEngine


class Chatting:
    """
    ìˆœìˆ˜í•œ ì„œë¹„ìŠ¤ ë¡œì§ë§Œ ë‹´ê³  ìžˆëŠ” í´ëž˜ìŠ¤ìž…ë‹ˆë‹¤. 
    LlmEngineì„ ì£¼ìž…ë°›ì•„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # ðŸŒŸ ìƒì„±ìžë¥¼ í†µí•´ LlmEngine ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì£¼ìž…ë°›ìŠµë‹ˆë‹¤.
    def __init__(self, llm_engine: LlmEngine):
        self.llm_engine = llm_engine
        print("ðŸ’¡ Chatting Class initialized with LlmEngine.")
        
    # ðŸŒŸðŸŒŸðŸŒŸ ëˆ„ë½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ì€ Getter ë©”ì„œë“œ ðŸŒŸðŸŒŸðŸŒŸ
    def get_llm_engine(self):
        """LlmEngine ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” Getter ë©”ì„œë“œ"""
        return self.llm_engine
    
    def get_gongo_service(self):
        """Gongo ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” Getter ë©”ì„œë“œ"""
        # LlmEngineì´ Gongoë¥¼ ê°€ì§€ê³  ìžˆìœ¼ë¯€ë¡œ LlmEngineì„ í†µí•´ ì ‘ê·¼í•©ë‹ˆë‹¤.
        return self.llm_engine.gongo_service
    # ðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸ

    async def get_chat_response(self, request: ChatRequest) -> ChatResponse:
        """
        ì‚¬ìš©ìžì˜ ìš”ì²­ì„ ë°›ì•„ LlmEngineì„ í˜¸ì¶œí•˜ê³  ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        # ðŸŒŸ LlmEngineì˜ generate_response ë©”ì„œë“œë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        llm_result = await self.llm_engine.generate_response(request)
        
        # LlmEngineì˜ ê²°ê³¼ë¥¼ ChatResponse í˜•ì‹ì— ë§žê²Œ ê°€ê³µí•©ë‹ˆë‹¤.
        final_response = llm_result.get("llm_output", "LLM ì‘ë‹µì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return ChatResponse(
            response=f"[LLM ì—”ì§„ ì²˜ë¦¬ ê²°ê³¼] {final_response}",
            status="llm_mock_processed",
            processed_by=f"Chatting -> LlmEngine (Used Tokens: {llm_result.get('usage_tokens')})"
        )