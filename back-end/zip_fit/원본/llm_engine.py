from typing import Dict, Any
import asyncio
from .models import ChatRequest 
# ğŸŒŸ Gongo ì„í¬íŠ¸
from .gongo import Gongo 

class LlmEngine:
    """
    LLM í˜¸ì¶œ, í”„ë¡¬í”„íŠ¸ êµ¬ì„±, LangChain/LangGraph ë“±ì˜ ì§€ëŠ¥í˜• ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    # ğŸŒŸ ìƒì„±ìë¥¼ í†µí•´ Gongo ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì£¼ì…ë°›ìŠµë‹ˆë‹¤.
    def __init__(self, gongo_service: Gongo):
        self.gongo_service = gongo_service
        print("âš™ï¸ LlmEngine Initialized with Gongo service.")

    # ----------------------------------------------------
    # ğŸŒŸ ìš”ì²­í•˜ì‹  ë©”ì„œë“œ 1: Gongoì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ì˜¤ëŠ” ë©”ì„œë“œ
    # ----------------------------------------------------
    async def _get_llm_input_text(self, request: ChatRequest) -> str:
        """
        Gongo ì„œë¹„ìŠ¤ì—ì„œ RAG ë° ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ LLM ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        # Gongo ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        context_data = await self.gongo_service.get_contextual_data(
            user_id=request.user_id, 
            query=request.user_input
        )
        
        # ìµœì¢…ì ìœ¼ë¡œ LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
        llm_input_text = (
            f"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n\n"
            f"{context_data}\n\n"
            f"ì‚¬ìš©ì ì§ˆë¬¸: {request.user_input}"
        )
        
        return llm_input_text

    # ----------------------------------------------------
    # ğŸŒŸ ìš”ì²­í•˜ì‹  ë©”ì„œë“œ 2: LLMì„ í˜¸ì¶œí•˜ëŠ” ë©”ì„œë“œ (Mock)
    # ----------------------------------------------------
    async def _call_llm_api(self, prompt_text: str) -> Dict[str, Any]:
        """
        OpenAI, LangChain ë“±ì„ ì´ìš©í•˜ì—¬ ì‹¤ì œ LLM APIë¥¼ í˜¸ì¶œí•˜ê³  ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤. (í˜„ì¬ëŠ” Mock)
        """
        # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´ ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤.
        await asyncio.sleep(0.05)
        
        # Mock ì‘ë‹µì„ êµ¬ì„±í•©ë‹ˆë‹¤.
        mock_llm_response = {
            "llm_output": f"LLMì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt_text)} ë¬¸ì)",
            "prompt_used": prompt_text,
            "usage_tokens": len(prompt_text) // 5 # ëŒ€ëµì ì¸ í† í° Mock
        }
        
        return mock_llm_response

    async def generate_response(self, request: ChatRequest) -> Dict[str, Any]:
        """
        Chatting í´ë˜ìŠ¤ì—ì„œ í˜¸ì¶œë˜ëŠ” ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œì…ë‹ˆë‹¤.
        """
        # 1. Gongoë¥¼ í†µí•´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        prompt_text = await self._get_llm_input_text(request)
        
        # 2. LLM í˜¸ì¶œ
        llm_result = await self._call_llm_api(prompt_text)
        
        return llm_result